{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D, concatenate\n",
    "from keras.layers import Input, Conv2DTranspose\n",
    "from keras.models import Model\n",
    "from ipykernel import kernelapp as app\n",
    "from keras import optimizers\n",
    "from keras.regularizers import l2\n",
    "#from keras import backend as K\n",
    "\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.utils import np_utils\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "import SimpleITK as sitk\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "%matplotlib inline \n",
    "import numpy as np\n",
    "import os\n",
    "import fnmatch\n",
    "import shutil\n",
    "from ipywidgets import interact, fixed\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from skimage import exposure\n",
    "from skimage import data\n",
    "from skimage.transform import rotate\n",
    "import _pickle as cPickle\n",
    "import random\n",
    "\n",
    "\n",
    "\n",
    "def myshow(image_arr,slice_n):\n",
    "    plt.imshow(image_arr[slice_n,:,:],cmap='gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_pickle('e://Image Processing/Brats Project/im+masks.pkl')\n",
    "\n",
    "def get_arrays(data, column=['X', 'Y']):\n",
    "    \n",
    "    x=np.array([i for i in data[column[0]]]) #array of images\n",
    "    y=np.array([i for i in data[column[1]]])#array for labels\n",
    "    \n",
    "    return x,y \n",
    "\n",
    "PATH_images='e://Image Processing/Brats Project/data/images/'\n",
    "PATH_masks='e://Image Processing/Brats Project/data/masks/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_cols=['X', 'Y_1', 'Y_2', 'Y_3']\n",
    "df.columns=new_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting arrays from columns \n",
    "\n",
    "train, test = train_test_split(df, test_size=0.20)\n",
    "\n",
    "x_train, y_train = get_arrays(train, column=['X', 'Y_1'])\n",
    "x_test, y_test = get_arrays(test, column=['X', 'Y_1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.cast(image, tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the training datasets\n",
    "dx_train = tf.data.Dataset.from_tensor_slices(tf.cast(x_train,tf.float32))\n",
    "\n",
    "# apply a one-hot transformation to each label for use in the neural network\n",
    "dy_train = tf.data.Dataset.from_tensor_slices(y_train).map(lambda z: tf.one_hot(z, 2))\n",
    "\n",
    "# zip the x and y training data together and shuffle, batch etc.\n",
    "train_dataset = tf.data.Dataset.zip((dx_train, dy_train)).shuffle(500).repeat().batch(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the same operations for the validation set\n",
    "dx_valid = tf.data.Dataset.from_tensor_slices(tf.cast(x_test,tf.float32))\n",
    "dy_valid = tf.data.Dataset.from_tensor_slices(y_test).map(lambda z: tf.one_hot(z, 2))\n",
    "valid_dataset = tf.data.Dataset.zip((dx_valid, dy_valid)).shuffle(500).repeat().batch(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create general iterator\n",
    "iterator = tf.data.Iterator.from_structure(train_dataset.output_types,\n",
    "                                               train_dataset.output_shapes)\n",
    "next_element = iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make datasets that we can initialize separately, but using the same structure via the common iterator\n",
    "training_init_op = iterator.make_initializer(train_dataset)\n",
    "validation_init_op = iterator.make_initializer(valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_model(in_data):\n",
    "    bn = tf.layers.batch_normalization(in_data)\n",
    "    fc1 = tf.layers.dense(bn, 50)\n",
    "    fc2 = tf.layers.dense(fc1, 50)\n",
    "    fc2 = tf.layers.dropout(fc2)\n",
    "    fc3 = tf.layers.dense(fc2, 2)\n",
    "    return fc3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the neural network model\n",
    "logits = nn_model(next_element[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the optimizer and loss\n",
    "loss = tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits_v2(labels=next_element[1], logits=logits))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(loss)\n",
    "# get accuracy\n",
    "prediction = tf.argmax(logits, 1)\n",
    "equality = tf.equal(prediction, tf.argmax(next_element[1], 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(equality, tf.float32))\n",
    "init_op = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, loss: 701446.500, training accuracy: 51.97%\n",
      "Epoch: 1, loss: 314900.312, training accuracy: 54.54%\n",
      "Epoch: 2, loss: 294328.000, training accuracy: 52.79%\n",
      "Epoch: 3, loss: 278959.375, training accuracy: 57.38%\n",
      "Epoch: 4, loss: 288051.875, training accuracy: 56.83%\n",
      "Epoch: 5, loss: 286902.500, training accuracy: 60.17%\n",
      "Epoch: 6, loss: 263858.719, training accuracy: 58.75%\n",
      "Epoch: 7, loss: 284543.750, training accuracy: 61.76%\n",
      "Epoch: 8, loss: 245585.812, training accuracy: 59.49%\n",
      "Epoch: 9, loss: 353572.938, training accuracy: 62.21%\n",
      "Epoch: 10, loss: 373610.875, training accuracy: 61.42%\n",
      "Epoch: 11, loss: 301123.562, training accuracy: 61.06%\n",
      "Epoch: 12, loss: 444187.125, training accuracy: 58.65%\n",
      "Epoch: 13, loss: 309188.281, training accuracy: 57.50%\n",
      "Epoch: 14, loss: 302347.344, training accuracy: 63.00%\n",
      "Epoch: 15, loss: 379097.812, training accuracy: 61.65%\n",
      "Epoch: 16, loss: 348732.625, training accuracy: 60.85%\n",
      "Epoch: 17, loss: 194252.188, training accuracy: 58.90%\n",
      "Epoch: 18, loss: 274744.438, training accuracy: 62.15%\n",
      "Epoch: 19, loss: 276185.625, training accuracy: 59.68%\n",
      "Epoch: 20, loss: 237953.219, training accuracy: 60.00%\n",
      "Epoch: 21, loss: 263232.344, training accuracy: 58.81%\n",
      "Epoch: 22, loss: 266049.469, training accuracy: 57.00%\n",
      "Epoch: 23, loss: 217405.719, training accuracy: 55.07%\n",
      "Epoch: 24, loss: 197942.469, training accuracy: 53.43%\n",
      "Epoch: 25, loss: 203800.875, training accuracy: 55.46%\n",
      "Epoch: 26, loss: 211609.094, training accuracy: 52.88%\n",
      "Epoch: 27, loss: 230646.562, training accuracy: 54.83%\n",
      "Epoch: 28, loss: 189766.156, training accuracy: 55.79%\n",
      "Epoch: 29, loss: 168961.812, training accuracy: 53.53%\n",
      "Epoch: 30, loss: 193051.750, training accuracy: 51.92%\n",
      "Epoch: 31, loss: 158194.656, training accuracy: 54.36%\n",
      "Epoch: 32, loss: 154324.250, training accuracy: 54.51%\n",
      "Epoch: 33, loss: 143153.719, training accuracy: 51.44%\n",
      "Epoch: 34, loss: 154142.906, training accuracy: 52.56%\n",
      "Epoch: 35, loss: 158548.250, training accuracy: 55.67%\n",
      "Epoch: 36, loss: 161427.219, training accuracy: 52.36%\n",
      "Epoch: 37, loss: 136667.688, training accuracy: 55.06%\n",
      "Epoch: 38, loss: 141354.859, training accuracy: 55.08%\n",
      "Epoch: 39, loss: 131911.609, training accuracy: 52.14%\n",
      "Epoch: 40, loss: 124670.320, training accuracy: 56.07%\n",
      "Epoch: 41, loss: 138141.750, training accuracy: 58.24%\n",
      "Epoch: 42, loss: 126329.492, training accuracy: 54.37%\n",
      "Epoch: 43, loss: 131136.844, training accuracy: 51.54%\n",
      "Epoch: 44, loss: 130061.656, training accuracy: 52.28%\n",
      "Epoch: 45, loss: 149992.406, training accuracy: 53.11%\n",
      "Epoch: 46, loss: 115408.422, training accuracy: 53.28%\n",
      "Epoch: 47, loss: 130190.055, training accuracy: 54.74%\n",
      "Epoch: 48, loss: 122425.195, training accuracy: 57.36%\n",
      "Epoch: 49, loss: 111698.672, training accuracy: 53.83%\n",
      "Epoch: 50, loss: 137217.844, training accuracy: 49.46%\n",
      "Epoch: 51, loss: 105807.188, training accuracy: 49.61%\n",
      "Epoch: 52, loss: 102128.797, training accuracy: 55.85%\n",
      "Epoch: 53, loss: 106984.547, training accuracy: 54.39%\n",
      "Epoch: 54, loss: 99108.781, training accuracy: 51.69%\n",
      "Epoch: 55, loss: 98707.586, training accuracy: 53.29%\n",
      "Epoch: 56, loss: 88826.047, training accuracy: 52.81%\n",
      "Epoch: 57, loss: 87759.469, training accuracy: 56.74%\n",
      "Epoch: 58, loss: 81482.031, training accuracy: 51.49%\n",
      "Epoch: 59, loss: 79109.500, training accuracy: 52.24%\n",
      "Epoch: 60, loss: 75718.766, training accuracy: 52.28%\n",
      "Epoch: 61, loss: 76306.000, training accuracy: 54.58%\n",
      "Epoch: 62, loss: 77416.141, training accuracy: 54.00%\n",
      "Epoch: 63, loss: 90716.070, training accuracy: 55.29%\n",
      "Epoch: 64, loss: 70871.094, training accuracy: 49.24%\n",
      "Epoch: 65, loss: 74815.617, training accuracy: 50.94%\n",
      "Epoch: 66, loss: 76107.383, training accuracy: 53.49%\n",
      "Epoch: 67, loss: 76132.203, training accuracy: 53.17%\n",
      "Epoch: 68, loss: 67532.758, training accuracy: 55.72%\n",
      "Epoch: 69, loss: 69744.000, training accuracy: 54.56%\n",
      "Epoch: 70, loss: 76215.469, training accuracy: 55.08%\n",
      "Epoch: 71, loss: 75696.188, training accuracy: 52.60%\n",
      "Epoch: 72, loss: 70820.984, training accuracy: 54.50%\n",
      "Epoch: 73, loss: 62902.766, training accuracy: 51.33%\n",
      "Epoch: 74, loss: 61262.789, training accuracy: 50.54%\n",
      "Epoch: 75, loss: 60837.871, training accuracy: 55.06%\n",
      "Epoch: 76, loss: 58336.984, training accuracy: 52.24%\n",
      "Epoch: 77, loss: 57081.895, training accuracy: 54.21%\n",
      "Epoch: 78, loss: 59789.496, training accuracy: 54.78%\n",
      "Epoch: 79, loss: 56803.668, training accuracy: 57.07%\n",
      "Epoch: 80, loss: 56740.559, training accuracy: 54.46%\n",
      "Epoch: 81, loss: 53174.812, training accuracy: 52.75%\n",
      "Epoch: 82, loss: 54509.359, training accuracy: 53.25%\n",
      "Epoch: 83, loss: 55554.469, training accuracy: 51.65%\n",
      "Epoch: 84, loss: 55212.703, training accuracy: 52.90%\n",
      "Epoch: 85, loss: 50444.102, training accuracy: 58.35%\n",
      "Epoch: 86, loss: 47866.055, training accuracy: 55.17%\n",
      "Epoch: 87, loss: 52813.773, training accuracy: 50.21%\n",
      "Epoch: 88, loss: 52605.207, training accuracy: 53.22%\n",
      "Epoch: 89, loss: 47160.633, training accuracy: 55.36%\n",
      "Epoch: 90, loss: 48411.555, training accuracy: 54.17%\n",
      "Epoch: 91, loss: 46509.656, training accuracy: 56.14%\n",
      "Epoch: 92, loss: 44949.695, training accuracy: 56.15%\n",
      "Epoch: 93, loss: 41905.902, training accuracy: 50.08%\n",
      "Epoch: 94, loss: 42118.469, training accuracy: 55.35%\n",
      "Epoch: 95, loss: 43744.840, training accuracy: 55.93%\n",
      "Epoch: 96, loss: 49651.328, training accuracy: 53.94%\n",
      "Epoch: 97, loss: 41275.633, training accuracy: 56.90%\n",
      "Epoch: 98, loss: 41276.898, training accuracy: 52.99%\n",
      "Epoch: 99, loss: 40493.219, training accuracy: 55.10%\n",
      "Epoch: 100, loss: 38550.664, training accuracy: 54.57%\n",
      "Epoch: 101, loss: 36882.602, training accuracy: 50.57%\n",
      "Epoch: 102, loss: 36789.266, training accuracy: 52.11%\n",
      "Epoch: 103, loss: 37348.516, training accuracy: 54.17%\n",
      "Epoch: 104, loss: 43293.398, training accuracy: 48.33%\n",
      "Epoch: 105, loss: 35101.844, training accuracy: 51.44%\n",
      "Epoch: 106, loss: 37289.852, training accuracy: 54.15%\n",
      "Epoch: 107, loss: 39861.453, training accuracy: 54.79%\n",
      "Epoch: 108, loss: 34731.211, training accuracy: 50.49%\n",
      "Epoch: 109, loss: 36449.336, training accuracy: 49.63%\n",
      "Epoch: 110, loss: 32231.312, training accuracy: 53.07%\n",
      "Epoch: 111, loss: 32263.199, training accuracy: 56.04%\n",
      "Epoch: 112, loss: 31469.006, training accuracy: 54.90%\n",
      "Epoch: 113, loss: 33871.836, training accuracy: 54.82%\n",
      "Epoch: 114, loss: 32208.492, training accuracy: 51.18%\n",
      "Epoch: 115, loss: 33023.586, training accuracy: 53.99%\n",
      "Epoch: 116, loss: 32016.939, training accuracy: 55.67%\n",
      "Epoch: 117, loss: 34678.348, training accuracy: 50.31%\n",
      "Epoch: 118, loss: 29586.691, training accuracy: 52.58%\n",
      "Epoch: 119, loss: 28077.660, training accuracy: 50.35%\n",
      "Epoch: 120, loss: 27658.932, training accuracy: 53.82%\n",
      "Epoch: 121, loss: 34982.598, training accuracy: 51.86%\n",
      "Epoch: 122, loss: 35020.445, training accuracy: 55.71%\n",
      "Epoch: 123, loss: 26115.543, training accuracy: 55.06%\n",
      "Epoch: 124, loss: 30144.641, training accuracy: 54.36%\n",
      "Epoch: 125, loss: 29816.879, training accuracy: 52.72%\n",
      "Epoch: 126, loss: 30132.457, training accuracy: 53.53%\n",
      "Epoch: 127, loss: 28248.902, training accuracy: 54.71%\n",
      "Epoch: 128, loss: 31242.863, training accuracy: 53.08%\n",
      "Epoch: 129, loss: 25584.266, training accuracy: 53.50%\n",
      "Epoch: 130, loss: 24431.320, training accuracy: 55.00%\n",
      "Epoch: 131, loss: 27985.115, training accuracy: 54.08%\n",
      "Epoch: 132, loss: 24771.104, training accuracy: 56.47%\n",
      "Epoch: 133, loss: 25182.086, training accuracy: 54.43%\n",
      "Epoch: 134, loss: 26120.793, training accuracy: 51.19%\n",
      "Epoch: 135, loss: 25867.578, training accuracy: 56.15%\n",
      "Epoch: 136, loss: 25042.902, training accuracy: 52.72%\n",
      "Epoch: 137, loss: 25783.793, training accuracy: 52.71%\n",
      "Epoch: 138, loss: 23395.656, training accuracy: 52.99%\n",
      "Epoch: 139, loss: 26900.883, training accuracy: 49.96%\n",
      "Epoch: 140, loss: 24407.770, training accuracy: 50.93%\n",
      "Epoch: 141, loss: 19923.748, training accuracy: 53.87%\n",
      "Epoch: 142, loss: 21228.219, training accuracy: 52.67%\n",
      "Epoch: 143, loss: 23754.275, training accuracy: 49.89%\n",
      "Epoch: 144, loss: 22745.914, training accuracy: 53.22%\n",
      "Epoch: 145, loss: 19455.787, training accuracy: 50.38%\n",
      "Epoch: 146, loss: 25599.822, training accuracy: 47.07%\n",
      "Epoch: 147, loss: 22938.320, training accuracy: 52.01%\n",
      "Epoch: 148, loss: 21571.301, training accuracy: 56.82%\n",
      "Epoch: 149, loss: 24626.512, training accuracy: 55.93%\n",
      "Epoch: 150, loss: 19993.137, training accuracy: 55.72%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 151, loss: 20975.836, training accuracy: 52.72%\n",
      "Epoch: 152, loss: 18582.627, training accuracy: 58.44%\n",
      "Epoch: 153, loss: 18193.635, training accuracy: 56.81%\n",
      "Epoch: 154, loss: 20254.488, training accuracy: 55.25%\n",
      "Epoch: 155, loss: 19222.180, training accuracy: 54.79%\n",
      "Epoch: 156, loss: 20674.105, training accuracy: 51.61%\n",
      "Epoch: 157, loss: 18304.244, training accuracy: 55.50%\n",
      "Epoch: 158, loss: 20549.088, training accuracy: 50.86%\n",
      "Epoch: 159, loss: 16012.146, training accuracy: 55.57%\n",
      "Epoch: 160, loss: 19954.613, training accuracy: 54.07%\n",
      "Epoch: 161, loss: 19174.496, training accuracy: 53.85%\n",
      "Epoch: 162, loss: 17195.912, training accuracy: 61.67%\n",
      "Epoch: 163, loss: 19531.492, training accuracy: 54.44%\n",
      "Epoch: 164, loss: 18759.152, training accuracy: 51.57%\n",
      "Epoch: 165, loss: 16607.559, training accuracy: 50.93%\n",
      "Epoch: 166, loss: 17141.072, training accuracy: 55.43%\n",
      "Epoch: 167, loss: 15588.971, training accuracy: 52.43%\n",
      "Epoch: 168, loss: 15850.978, training accuracy: 52.36%\n",
      "Epoch: 169, loss: 18088.414, training accuracy: 56.68%\n",
      "Epoch: 170, loss: 19429.664, training accuracy: 50.01%\n",
      "Epoch: 171, loss: 18546.896, training accuracy: 53.86%\n",
      "Epoch: 172, loss: 17564.457, training accuracy: 53.40%\n",
      "Epoch: 173, loss: 13670.841, training accuracy: 57.21%\n",
      "Epoch: 174, loss: 22843.701, training accuracy: 49.82%\n",
      "Epoch: 175, loss: 18196.355, training accuracy: 51.72%\n",
      "Epoch: 176, loss: 18862.451, training accuracy: 55.49%\n",
      "Epoch: 177, loss: 18542.734, training accuracy: 53.67%\n",
      "Epoch: 178, loss: 18813.469, training accuracy: 55.87%\n",
      "Epoch: 179, loss: 20869.398, training accuracy: 51.33%\n",
      "Epoch: 180, loss: 27053.547, training accuracy: 53.72%\n",
      "Epoch: 181, loss: 18071.234, training accuracy: 54.04%\n",
      "Epoch: 182, loss: 21375.219, training accuracy: 53.87%\n",
      "Epoch: 183, loss: 32291.658, training accuracy: 54.72%\n",
      "Epoch: 184, loss: 15753.926, training accuracy: 56.04%\n",
      "Epoch: 185, loss: 51287.508, training accuracy: 50.63%\n",
      "Epoch: 186, loss: 41611.641, training accuracy: 51.76%\n",
      "Epoch: 187, loss: 20256.977, training accuracy: 52.28%\n",
      "Epoch: 188, loss: 38812.691, training accuracy: 60.35%\n",
      "Epoch: 189, loss: 11885.514, training accuracy: 55.65%\n",
      "Epoch: 190, loss: 21100.723, training accuracy: 51.44%\n",
      "Epoch: 191, loss: 18108.930, training accuracy: 52.83%\n",
      "Epoch: 192, loss: 34254.449, training accuracy: 52.51%\n",
      "Epoch: 193, loss: 32505.398, training accuracy: 52.14%\n",
      "Epoch: 194, loss: 39686.406, training accuracy: 51.24%\n",
      "Epoch: 195, loss: 42236.184, training accuracy: 50.72%\n",
      "Epoch: 196, loss: 21355.621, training accuracy: 52.90%\n",
      "Epoch: 197, loss: 22827.406, training accuracy: 54.65%\n",
      "Epoch: 198, loss: 20691.830, training accuracy: 51.90%\n",
      "Epoch: 199, loss: 19485.635, training accuracy: 53.61%\n",
      "Epoch: 200, loss: 14331.912, training accuracy: 52.71%\n",
      "Epoch: 201, loss: 23716.842, training accuracy: 52.33%\n",
      "Epoch: 202, loss: 24019.379, training accuracy: 52.14%\n",
      "Epoch: 203, loss: 37188.410, training accuracy: 51.10%\n",
      "Epoch: 204, loss: 15323.157, training accuracy: 54.96%\n",
      "Epoch: 205, loss: 23126.479, training accuracy: 55.40%\n",
      "Epoch: 206, loss: 15155.744, training accuracy: 57.18%\n",
      "Epoch: 207, loss: 25816.566, training accuracy: 51.10%\n",
      "Epoch: 208, loss: 45104.492, training accuracy: 54.74%\n",
      "Epoch: 209, loss: 59560.070, training accuracy: 52.00%\n",
      "Epoch: 210, loss: 13043.465, training accuracy: 51.10%\n",
      "Epoch: 211, loss: 40424.500, training accuracy: 52.75%\n",
      "Epoch: 212, loss: 77343.578, training accuracy: 53.11%\n",
      "Epoch: 213, loss: 38962.363, training accuracy: 52.43%\n",
      "Epoch: 214, loss: 23957.686, training accuracy: 53.62%\n",
      "Epoch: 215, loss: 24585.004, training accuracy: 55.51%\n",
      "Epoch: 216, loss: 37610.730, training accuracy: 55.11%\n",
      "Epoch: 217, loss: 14061.525, training accuracy: 52.97%\n",
      "Epoch: 218, loss: 24487.301, training accuracy: 53.37%\n",
      "Epoch: 219, loss: 27547.682, training accuracy: 53.10%\n",
      "Epoch: 220, loss: 26358.416, training accuracy: 54.93%\n",
      "Epoch: 221, loss: 18475.617, training accuracy: 52.79%\n",
      "Epoch: 222, loss: 23577.822, training accuracy: 54.50%\n",
      "Epoch: 223, loss: 21950.141, training accuracy: 51.44%\n",
      "Epoch: 224, loss: 22080.023, training accuracy: 54.94%\n",
      "Epoch: 225, loss: 42180.172, training accuracy: 52.67%\n",
      "Epoch: 226, loss: 12742.293, training accuracy: 52.22%\n",
      "Epoch: 227, loss: 14187.296, training accuracy: 58.47%\n",
      "Epoch: 228, loss: 21842.330, training accuracy: 51.53%\n",
      "Epoch: 229, loss: 13306.511, training accuracy: 48.89%\n",
      "Epoch: 230, loss: 10150.099, training accuracy: 54.36%\n",
      "Epoch: 231, loss: 12992.061, training accuracy: 52.50%\n",
      "Epoch: 232, loss: 17565.508, training accuracy: 53.28%\n",
      "Epoch: 233, loss: 18221.766, training accuracy: 55.79%\n",
      "Epoch: 234, loss: 13782.433, training accuracy: 54.22%\n",
      "Epoch: 235, loss: 14108.758, training accuracy: 50.88%\n",
      "Epoch: 236, loss: 14261.010, training accuracy: 56.67%\n",
      "Epoch: 237, loss: 10048.691, training accuracy: 55.96%\n",
      "Epoch: 238, loss: 15276.190, training accuracy: 51.79%\n",
      "Epoch: 239, loss: 16192.368, training accuracy: 53.85%\n",
      "Epoch: 240, loss: 12448.543, training accuracy: 55.44%\n",
      "Epoch: 241, loss: 11877.209, training accuracy: 59.43%\n",
      "Epoch: 242, loss: 8703.507, training accuracy: 52.83%\n",
      "Epoch: 243, loss: 18648.205, training accuracy: 52.26%\n",
      "Epoch: 244, loss: 9657.391, training accuracy: 50.60%\n",
      "Epoch: 245, loss: 12778.193, training accuracy: 53.42%\n",
      "Epoch: 246, loss: 13576.831, training accuracy: 54.40%\n",
      "Epoch: 247, loss: 15544.910, training accuracy: 51.51%\n",
      "Epoch: 248, loss: 19119.582, training accuracy: 52.89%\n",
      "Epoch: 249, loss: 9716.100, training accuracy: 51.60%\n",
      "Epoch: 250, loss: 12366.556, training accuracy: 52.21%\n",
      "Epoch: 251, loss: 10423.310, training accuracy: 54.72%\n",
      "Epoch: 252, loss: 10362.699, training accuracy: 52.75%\n",
      "Epoch: 253, loss: 11383.312, training accuracy: 55.28%\n",
      "Epoch: 254, loss: 13870.527, training accuracy: 52.96%\n",
      "Epoch: 255, loss: 12173.801, training accuracy: 52.04%\n",
      "Epoch: 256, loss: 13905.760, training accuracy: 53.33%\n",
      "Epoch: 257, loss: 10767.679, training accuracy: 53.79%\n",
      "Epoch: 258, loss: 11457.797, training accuracy: 54.03%\n",
      "Epoch: 259, loss: 9894.205, training accuracy: 50.74%\n",
      "Epoch: 260, loss: 15805.719, training accuracy: 56.64%\n",
      "Epoch: 261, loss: 8872.117, training accuracy: 54.26%\n",
      "Epoch: 262, loss: 11298.472, training accuracy: 53.40%\n",
      "Epoch: 263, loss: 12677.597, training accuracy: 50.31%\n",
      "Epoch: 264, loss: 13354.015, training accuracy: 58.39%\n",
      "Epoch: 265, loss: 9733.535, training accuracy: 53.53%\n",
      "Epoch: 266, loss: 10861.637, training accuracy: 53.42%\n",
      "Epoch: 267, loss: 9950.650, training accuracy: 56.57%\n",
      "Epoch: 268, loss: 10824.071, training accuracy: 53.65%\n",
      "Epoch: 269, loss: 8485.217, training accuracy: 54.17%\n",
      "Epoch: 270, loss: 8147.090, training accuracy: 55.97%\n",
      "Epoch: 271, loss: 9401.838, training accuracy: 56.06%\n",
      "Epoch: 272, loss: 10794.127, training accuracy: 54.06%\n",
      "Epoch: 273, loss: 6577.187, training accuracy: 55.25%\n",
      "Epoch: 274, loss: 10220.750, training accuracy: 49.07%\n",
      "Epoch: 275, loss: 7598.274, training accuracy: 52.64%\n",
      "Epoch: 276, loss: 9790.390, training accuracy: 54.68%\n",
      "Epoch: 277, loss: 9027.713, training accuracy: 54.01%\n",
      "Epoch: 278, loss: 11413.604, training accuracy: 53.12%\n",
      "Epoch: 279, loss: 8034.337, training accuracy: 51.53%\n",
      "Epoch: 280, loss: 8428.703, training accuracy: 51.79%\n",
      "Epoch: 281, loss: 13655.298, training accuracy: 51.71%\n",
      "Epoch: 282, loss: 9240.222, training accuracy: 55.43%\n",
      "Epoch: 283, loss: 8649.855, training accuracy: 55.83%\n",
      "Epoch: 284, loss: 7264.465, training accuracy: 49.72%\n",
      "Epoch: 285, loss: 9425.190, training accuracy: 56.32%\n",
      "Epoch: 286, loss: 10228.977, training accuracy: 50.35%\n",
      "Epoch: 287, loss: 17443.094, training accuracy: 53.97%\n",
      "Epoch: 288, loss: 8502.757, training accuracy: 55.97%\n",
      "Epoch: 289, loss: 10865.127, training accuracy: 55.40%\n",
      "Epoch: 290, loss: 11817.741, training accuracy: 50.99%\n",
      "Epoch: 291, loss: 18269.512, training accuracy: 54.33%\n",
      "Epoch: 292, loss: 13666.809, training accuracy: 51.78%\n",
      "Epoch: 293, loss: 13796.471, training accuracy: 49.15%\n",
      "Epoch: 294, loss: 16285.317, training accuracy: 53.82%\n",
      "Epoch: 295, loss: 8867.050, training accuracy: 55.81%\n",
      "Epoch: 296, loss: 8968.277, training accuracy: 51.22%\n",
      "Epoch: 297, loss: 10412.031, training accuracy: 54.25%\n",
      "Epoch: 298, loss: 14698.724, training accuracy: 55.51%\n",
      "Epoch: 299, loss: 14393.287, training accuracy: 54.26%\n",
      "Epoch: 300, loss: 6168.811, training accuracy: 57.32%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 301, loss: 11073.584, training accuracy: 54.33%\n",
      "Epoch: 302, loss: 7432.574, training accuracy: 50.31%\n",
      "Epoch: 303, loss: 10046.992, training accuracy: 53.40%\n",
      "Epoch: 304, loss: 15195.010, training accuracy: 52.42%\n",
      "Epoch: 305, loss: 10720.969, training accuracy: 56.72%\n",
      "Epoch: 306, loss: 13891.373, training accuracy: 54.57%\n",
      "Epoch: 307, loss: 10197.762, training accuracy: 52.65%\n",
      "Epoch: 308, loss: 20833.551, training accuracy: 54.15%\n",
      "Epoch: 309, loss: 10918.387, training accuracy: 55.25%\n",
      "Epoch: 310, loss: 20042.098, training accuracy: 52.03%\n",
      "Epoch: 311, loss: 28958.064, training accuracy: 54.44%\n",
      "Epoch: 312, loss: 17273.453, training accuracy: 55.53%\n",
      "Epoch: 313, loss: 13695.285, training accuracy: 56.08%\n",
      "Epoch: 314, loss: 4939.195, training accuracy: 57.53%\n",
      "Epoch: 315, loss: 14995.326, training accuracy: 51.10%\n",
      "Epoch: 316, loss: 7455.153, training accuracy: 53.10%\n",
      "Epoch: 317, loss: 7900.073, training accuracy: 56.61%\n",
      "Epoch: 318, loss: 11198.170, training accuracy: 57.29%\n",
      "Epoch: 319, loss: 8481.601, training accuracy: 54.42%\n",
      "Epoch: 320, loss: 15442.646, training accuracy: 50.81%\n",
      "Epoch: 321, loss: 10184.413, training accuracy: 53.10%\n",
      "Epoch: 322, loss: 10754.516, training accuracy: 51.40%\n",
      "Epoch: 323, loss: 8319.226, training accuracy: 55.51%\n",
      "Epoch: 324, loss: 9015.311, training accuracy: 55.33%\n",
      "Epoch: 325, loss: 11166.045, training accuracy: 57.47%\n",
      "Epoch: 326, loss: 11803.330, training accuracy: 51.57%\n",
      "Epoch: 327, loss: 11873.818, training accuracy: 52.46%\n",
      "Epoch: 328, loss: 12134.951, training accuracy: 52.51%\n",
      "Epoch: 329, loss: 10440.207, training accuracy: 52.26%\n",
      "Epoch: 330, loss: 9504.995, training accuracy: 55.15%\n",
      "Epoch: 331, loss: 11298.963, training accuracy: 53.28%\n",
      "Epoch: 332, loss: 8676.621, training accuracy: 54.07%\n",
      "Epoch: 333, loss: 14470.148, training accuracy: 50.86%\n",
      "Epoch: 334, loss: 7822.794, training accuracy: 54.93%\n",
      "Epoch: 335, loss: 7045.206, training accuracy: 53.18%\n",
      "Epoch: 336, loss: 9531.387, training accuracy: 54.26%\n",
      "Epoch: 337, loss: 6695.414, training accuracy: 57.88%\n",
      "Epoch: 338, loss: 9963.736, training accuracy: 55.42%\n",
      "Epoch: 339, loss: 6559.899, training accuracy: 52.49%\n",
      "Epoch: 340, loss: 10251.944, training accuracy: 51.71%\n",
      "Epoch: 341, loss: 8123.945, training accuracy: 51.26%\n",
      "Epoch: 342, loss: 8930.360, training accuracy: 54.21%\n",
      "Epoch: 343, loss: 8210.243, training accuracy: 54.83%\n",
      "Epoch: 344, loss: 8149.699, training accuracy: 50.90%\n",
      "Epoch: 345, loss: 9507.689, training accuracy: 52.18%\n",
      "Epoch: 346, loss: 6510.786, training accuracy: 52.25%\n",
      "Epoch: 347, loss: 6374.761, training accuracy: 52.67%\n",
      "Epoch: 348, loss: 9642.393, training accuracy: 50.88%\n",
      "Epoch: 349, loss: 7685.651, training accuracy: 51.56%\n",
      "Epoch: 350, loss: 6884.634, training accuracy: 53.17%\n",
      "Epoch: 351, loss: 7706.219, training accuracy: 54.47%\n",
      "Epoch: 352, loss: 8921.604, training accuracy: 50.61%\n",
      "Epoch: 353, loss: 6904.651, training accuracy: 54.24%\n",
      "Epoch: 354, loss: 5919.022, training accuracy: 52.97%\n",
      "Epoch: 355, loss: 8594.315, training accuracy: 53.29%\n",
      "Epoch: 356, loss: 12753.793, training accuracy: 51.42%\n",
      "Epoch: 357, loss: 7646.347, training accuracy: 53.49%\n",
      "Epoch: 358, loss: 17448.453, training accuracy: 56.60%\n",
      "Epoch: 359, loss: 8377.207, training accuracy: 55.97%\n",
      "Epoch: 360, loss: 15683.276, training accuracy: 55.62%\n",
      "Epoch: 361, loss: 10882.623, training accuracy: 54.12%\n",
      "Epoch: 362, loss: 40645.531, training accuracy: 52.19%\n",
      "Epoch: 363, loss: 9534.201, training accuracy: 49.65%\n",
      "Epoch: 364, loss: 12160.591, training accuracy: 53.83%\n",
      "Epoch: 365, loss: 7882.393, training accuracy: 52.51%\n",
      "Epoch: 366, loss: 6127.069, training accuracy: 57.39%\n",
      "Epoch: 367, loss: 16039.740, training accuracy: 50.42%\n",
      "Epoch: 368, loss: 4591.878, training accuracy: 54.14%\n",
      "Epoch: 369, loss: 20979.199, training accuracy: 52.74%\n",
      "Epoch: 370, loss: 16102.324, training accuracy: 55.58%\n",
      "Epoch: 371, loss: 21385.752, training accuracy: 49.74%\n",
      "Epoch: 372, loss: 15186.322, training accuracy: 51.51%\n",
      "Epoch: 373, loss: 14525.951, training accuracy: 54.08%\n",
      "Epoch: 374, loss: 7453.574, training accuracy: 49.31%\n",
      "Epoch: 375, loss: 23168.297, training accuracy: 51.29%\n",
      "Epoch: 376, loss: 8341.078, training accuracy: 54.75%\n",
      "Epoch: 377, loss: 4798.453, training accuracy: 55.56%\n",
      "Epoch: 378, loss: 33129.781, training accuracy: 54.92%\n",
      "Epoch: 379, loss: 14850.904, training accuracy: 54.69%\n",
      "Epoch: 380, loss: 22857.234, training accuracy: 49.75%\n",
      "Epoch: 381, loss: 3563.419, training accuracy: 60.50%\n",
      "Epoch: 382, loss: 13674.058, training accuracy: 54.56%\n",
      "Epoch: 383, loss: 5291.891, training accuracy: 59.07%\n",
      "Epoch: 384, loss: 11231.717, training accuracy: 53.22%\n",
      "Epoch: 385, loss: 10106.262, training accuracy: 52.13%\n",
      "Epoch: 386, loss: 8392.951, training accuracy: 48.57%\n",
      "Epoch: 387, loss: 12748.990, training accuracy: 53.19%\n",
      "Epoch: 388, loss: 15530.079, training accuracy: 52.75%\n",
      "Epoch: 389, loss: 6650.281, training accuracy: 54.74%\n",
      "Epoch: 390, loss: 11235.037, training accuracy: 56.14%\n",
      "Epoch: 391, loss: 7844.441, training accuracy: 52.17%\n",
      "Epoch: 392, loss: 6267.888, training accuracy: 53.57%\n",
      "Epoch: 393, loss: 9003.882, training accuracy: 51.82%\n",
      "Epoch: 394, loss: 8227.656, training accuracy: 53.49%\n",
      "Epoch: 395, loss: 6120.653, training accuracy: 53.10%\n",
      "Epoch: 396, loss: 7539.529, training accuracy: 54.53%\n",
      "Epoch: 397, loss: 9127.253, training accuracy: 56.07%\n",
      "Epoch: 398, loss: 11830.166, training accuracy: 54.64%\n",
      "Epoch: 399, loss: 9235.691, training accuracy: 51.94%\n",
      "Epoch: 400, loss: 8650.680, training accuracy: 51.65%\n",
      "Epoch: 401, loss: 3853.279, training accuracy: 55.28%\n",
      "Epoch: 402, loss: 11235.396, training accuracy: 51.76%\n",
      "Epoch: 403, loss: 9849.482, training accuracy: 51.06%\n",
      "Epoch: 404, loss: 6564.167, training accuracy: 54.37%\n",
      "Epoch: 405, loss: 10523.695, training accuracy: 52.25%\n",
      "Epoch: 406, loss: 7536.734, training accuracy: 53.53%\n",
      "Epoch: 407, loss: 5292.648, training accuracy: 53.17%\n",
      "Epoch: 408, loss: 5914.271, training accuracy: 53.67%\n",
      "Epoch: 409, loss: 6621.141, training accuracy: 56.53%\n",
      "Epoch: 410, loss: 8941.345, training accuracy: 53.33%\n",
      "Epoch: 411, loss: 3605.918, training accuracy: 51.75%\n",
      "Epoch: 412, loss: 6666.215, training accuracy: 53.40%\n",
      "Epoch: 413, loss: 7817.404, training accuracy: 58.64%\n",
      "Epoch: 414, loss: 6513.155, training accuracy: 54.47%\n",
      "Epoch: 415, loss: 8233.408, training accuracy: 53.43%\n",
      "Epoch: 416, loss: 12025.020, training accuracy: 53.93%\n",
      "Epoch: 417, loss: 8175.705, training accuracy: 54.82%\n",
      "Epoch: 418, loss: 6492.269, training accuracy: 54.82%\n",
      "Epoch: 419, loss: 6642.538, training accuracy: 52.17%\n",
      "Epoch: 420, loss: 9047.801, training accuracy: 50.65%\n",
      "Epoch: 421, loss: 9802.787, training accuracy: 51.33%\n",
      "Epoch: 422, loss: 8417.956, training accuracy: 52.61%\n",
      "Epoch: 423, loss: 16907.693, training accuracy: 51.36%\n",
      "Epoch: 424, loss: 8419.992, training accuracy: 54.75%\n",
      "Epoch: 425, loss: 7903.216, training accuracy: 54.12%\n",
      "Epoch: 426, loss: 28606.805, training accuracy: 53.94%\n",
      "Epoch: 427, loss: 14770.316, training accuracy: 50.63%\n",
      "Epoch: 428, loss: 13320.064, training accuracy: 58.42%\n",
      "Epoch: 429, loss: 11881.658, training accuracy: 51.69%\n",
      "Epoch: 430, loss: 13894.756, training accuracy: 51.32%\n",
      "Epoch: 431, loss: 16073.707, training accuracy: 58.60%\n",
      "Epoch: 432, loss: 9097.888, training accuracy: 54.49%\n",
      "Epoch: 433, loss: 6778.146, training accuracy: 54.33%\n",
      "Epoch: 434, loss: 6593.211, training accuracy: 51.53%\n",
      "Epoch: 435, loss: 9742.269, training accuracy: 50.58%\n",
      "Epoch: 436, loss: 6418.164, training accuracy: 54.40%\n",
      "Epoch: 437, loss: 9489.571, training accuracy: 52.49%\n",
      "Epoch: 438, loss: 5515.681, training accuracy: 55.12%\n",
      "Epoch: 439, loss: 13492.070, training accuracy: 50.44%\n",
      "Epoch: 440, loss: 8329.871, training accuracy: 54.00%\n",
      "Epoch: 441, loss: 9952.097, training accuracy: 54.56%\n",
      "Epoch: 442, loss: 40270.844, training accuracy: 53.65%\n",
      "Epoch: 443, loss: 20621.637, training accuracy: 52.88%\n",
      "Epoch: 444, loss: 6355.598, training accuracy: 52.21%\n",
      "Epoch: 445, loss: 3380.969, training accuracy: 57.42%\n",
      "Epoch: 446, loss: 9887.528, training accuracy: 54.54%\n",
      "Epoch: 447, loss: 12395.195, training accuracy: 53.03%\n",
      "Epoch: 448, loss: 9096.217, training accuracy: 50.21%\n",
      "Epoch: 449, loss: 12601.895, training accuracy: 57.11%\n",
      "Epoch: 450, loss: 8739.354, training accuracy: 52.15%\n",
      "Epoch: 451, loss: 13539.104, training accuracy: 51.88%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 452, loss: 7071.744, training accuracy: 54.39%\n",
      "Epoch: 453, loss: 9804.426, training accuracy: 54.33%\n",
      "Epoch: 454, loss: 9706.312, training accuracy: 53.65%\n",
      "Epoch: 455, loss: 7924.737, training accuracy: 53.87%\n",
      "Epoch: 456, loss: 11490.307, training accuracy: 50.81%\n",
      "Epoch: 457, loss: 15450.517, training accuracy: 50.90%\n",
      "Epoch: 458, loss: 15335.723, training accuracy: 54.68%\n",
      "Epoch: 459, loss: 6437.358, training accuracy: 55.21%\n",
      "Epoch: 460, loss: 23707.605, training accuracy: 48.82%\n",
      "Epoch: 461, loss: 10096.745, training accuracy: 55.32%\n",
      "Epoch: 462, loss: 17768.771, training accuracy: 58.19%\n",
      "Epoch: 463, loss: 11285.680, training accuracy: 55.94%\n",
      "Epoch: 464, loss: 29976.156, training accuracy: 54.03%\n",
      "Epoch: 465, loss: 84285.797, training accuracy: 54.36%\n",
      "Epoch: 466, loss: 36817.043, training accuracy: 53.12%\n",
      "Epoch: 467, loss: 56715.629, training accuracy: 51.43%\n",
      "Epoch: 468, loss: 27679.414, training accuracy: 51.58%\n",
      "Epoch: 469, loss: 52899.727, training accuracy: 50.54%\n",
      "Epoch: 470, loss: 61195.102, training accuracy: 51.03%\n",
      "Epoch: 471, loss: 45166.035, training accuracy: 53.64%\n",
      "Epoch: 472, loss: 35485.117, training accuracy: 51.68%\n",
      "Epoch: 473, loss: 23663.293, training accuracy: 57.04%\n",
      "Epoch: 474, loss: 18797.324, training accuracy: 50.26%\n",
      "Epoch: 475, loss: 7605.481, training accuracy: 53.00%\n",
      "Epoch: 476, loss: 16709.188, training accuracy: 53.29%\n",
      "Epoch: 477, loss: 6970.035, training accuracy: 50.97%\n",
      "Epoch: 478, loss: 12802.531, training accuracy: 52.60%\n",
      "Epoch: 479, loss: 18861.117, training accuracy: 51.83%\n",
      "Epoch: 480, loss: 24030.096, training accuracy: 52.60%\n",
      "Epoch: 481, loss: 14362.044, training accuracy: 51.72%\n",
      "Epoch: 482, loss: 27045.535, training accuracy: 56.39%\n",
      "Epoch: 483, loss: 22330.480, training accuracy: 53.71%\n",
      "Epoch: 484, loss: 10454.076, training accuracy: 50.54%\n",
      "Epoch: 485, loss: 11275.957, training accuracy: 59.21%\n",
      "Epoch: 486, loss: 11997.235, training accuracy: 50.54%\n",
      "Epoch: 487, loss: 5506.905, training accuracy: 56.96%\n",
      "Epoch: 488, loss: 11265.395, training accuracy: 56.25%\n",
      "Epoch: 489, loss: 6525.781, training accuracy: 53.90%\n",
      "Epoch: 490, loss: 5377.362, training accuracy: 53.18%\n",
      "Epoch: 491, loss: 4872.020, training accuracy: 52.26%\n",
      "Epoch: 492, loss: 11558.412, training accuracy: 56.60%\n",
      "Epoch: 493, loss: 11229.118, training accuracy: 52.53%\n",
      "Epoch: 494, loss: 7718.623, training accuracy: 53.10%\n",
      "Epoch: 495, loss: 10623.285, training accuracy: 55.37%\n",
      "Epoch: 496, loss: 11672.574, training accuracy: 54.19%\n",
      "Epoch: 497, loss: 5609.545, training accuracy: 53.51%\n",
      "Epoch: 498, loss: 5991.532, training accuracy: 55.12%\n",
      "Epoch: 499, loss: 2305.398, training accuracy: 56.44%\n",
      "Epoch: 500, loss: 13889.758, training accuracy: 55.94%\n",
      "Epoch: 501, loss: 18334.289, training accuracy: 54.46%\n",
      "Epoch: 502, loss: 11553.712, training accuracy: 55.99%\n",
      "Epoch: 503, loss: 10910.711, training accuracy: 56.69%\n",
      "Epoch: 504, loss: 12128.684, training accuracy: 51.36%\n",
      "Epoch: 505, loss: 10322.436, training accuracy: 53.51%\n",
      "Epoch: 506, loss: 29477.117, training accuracy: 52.89%\n",
      "Epoch: 507, loss: 27234.434, training accuracy: 55.60%\n",
      "Epoch: 508, loss: 16237.145, training accuracy: 53.42%\n",
      "Epoch: 509, loss: 2884.382, training accuracy: 54.65%\n",
      "Epoch: 510, loss: 21063.711, training accuracy: 51.22%\n",
      "Epoch: 511, loss: 13328.734, training accuracy: 51.90%\n",
      "Epoch: 512, loss: 12925.350, training accuracy: 52.93%\n",
      "Epoch: 513, loss: 4606.436, training accuracy: 53.87%\n",
      "Epoch: 514, loss: 12229.867, training accuracy: 49.44%\n",
      "Epoch: 515, loss: 14290.633, training accuracy: 53.75%\n",
      "Epoch: 516, loss: 7546.149, training accuracy: 55.81%\n",
      "Epoch: 517, loss: 15087.494, training accuracy: 56.67%\n",
      "Epoch: 518, loss: 7175.304, training accuracy: 55.79%\n",
      "Epoch: 519, loss: 6549.375, training accuracy: 55.25%\n",
      "Epoch: 520, loss: 28453.135, training accuracy: 54.68%\n",
      "Epoch: 521, loss: 24992.059, training accuracy: 49.82%\n",
      "Epoch: 522, loss: 25921.779, training accuracy: 53.18%\n",
      "Epoch: 523, loss: 9443.638, training accuracy: 51.50%\n",
      "Epoch: 524, loss: 9500.869, training accuracy: 53.44%\n",
      "Epoch: 525, loss: 13741.676, training accuracy: 51.43%\n",
      "Epoch: 526, loss: 9656.217, training accuracy: 52.96%\n",
      "Epoch: 527, loss: 11258.424, training accuracy: 53.49%\n",
      "Epoch: 528, loss: 11393.746, training accuracy: 49.17%\n",
      "Epoch: 529, loss: 13755.826, training accuracy: 52.76%\n",
      "Epoch: 530, loss: 8966.336, training accuracy: 52.36%\n",
      "Epoch: 531, loss: 13636.145, training accuracy: 52.67%\n",
      "Epoch: 532, loss: 12076.102, training accuracy: 58.44%\n",
      "Epoch: 533, loss: 8676.436, training accuracy: 54.82%\n",
      "Epoch: 534, loss: 8613.176, training accuracy: 52.81%\n",
      "Epoch: 535, loss: 12591.969, training accuracy: 50.64%\n",
      "Epoch: 536, loss: 9817.414, training accuracy: 53.86%\n",
      "Epoch: 537, loss: 6823.469, training accuracy: 52.71%\n",
      "Epoch: 538, loss: 13759.284, training accuracy: 51.97%\n",
      "Epoch: 539, loss: 3723.544, training accuracy: 55.87%\n",
      "Epoch: 540, loss: 7211.897, training accuracy: 53.19%\n",
      "Epoch: 541, loss: 18740.523, training accuracy: 51.19%\n",
      "Epoch: 542, loss: 22061.848, training accuracy: 52.28%\n",
      "Epoch: 543, loss: 6898.089, training accuracy: 55.58%\n",
      "Epoch: 544, loss: 20418.023, training accuracy: 52.40%\n",
      "Epoch: 545, loss: 3981.023, training accuracy: 53.28%\n",
      "Epoch: 546, loss: 7731.294, training accuracy: 50.83%\n",
      "Epoch: 547, loss: 18149.893, training accuracy: 51.61%\n",
      "Epoch: 548, loss: 6937.370, training accuracy: 54.07%\n",
      "Epoch: 549, loss: 16675.266, training accuracy: 50.22%\n",
      "Epoch: 550, loss: 14450.084, training accuracy: 51.04%\n",
      "Epoch: 551, loss: 11119.783, training accuracy: 57.32%\n",
      "Epoch: 552, loss: 11993.555, training accuracy: 50.81%\n",
      "Epoch: 553, loss: 7124.649, training accuracy: 54.78%\n",
      "Epoch: 554, loss: 10819.133, training accuracy: 53.83%\n",
      "Epoch: 555, loss: 8039.479, training accuracy: 53.42%\n",
      "Epoch: 556, loss: 5909.148, training accuracy: 60.36%\n",
      "Epoch: 557, loss: 11187.543, training accuracy: 51.25%\n",
      "Epoch: 558, loss: 3725.980, training accuracy: 55.50%\n",
      "Epoch: 559, loss: 7014.680, training accuracy: 51.15%\n",
      "Epoch: 560, loss: 6960.194, training accuracy: 53.06%\n",
      "Epoch: 561, loss: 7173.014, training accuracy: 51.57%\n",
      "Epoch: 562, loss: 5204.124, training accuracy: 54.99%\n",
      "Epoch: 563, loss: 10079.168, training accuracy: 53.10%\n",
      "Epoch: 564, loss: 6196.481, training accuracy: 53.47%\n",
      "Epoch: 565, loss: 3563.746, training accuracy: 51.68%\n",
      "Epoch: 566, loss: 8981.616, training accuracy: 57.32%\n",
      "Epoch: 567, loss: 5646.139, training accuracy: 53.79%\n",
      "Epoch: 568, loss: 5406.353, training accuracy: 56.08%\n",
      "Epoch: 569, loss: 5796.359, training accuracy: 54.47%\n",
      "Epoch: 570, loss: 5469.251, training accuracy: 51.75%\n",
      "Epoch: 571, loss: 5725.632, training accuracy: 52.89%\n",
      "Epoch: 572, loss: 6094.199, training accuracy: 52.10%\n",
      "Epoch: 573, loss: 8203.867, training accuracy: 54.61%\n",
      "Epoch: 574, loss: 6685.158, training accuracy: 53.36%\n",
      "Epoch: 575, loss: 6119.429, training accuracy: 51.99%\n",
      "Epoch: 576, loss: 9681.270, training accuracy: 53.86%\n",
      "Epoch: 577, loss: 10278.503, training accuracy: 50.14%\n",
      "Epoch: 578, loss: 5118.931, training accuracy: 56.74%\n",
      "Epoch: 579, loss: 7524.947, training accuracy: 51.15%\n",
      "Epoch: 580, loss: 9132.104, training accuracy: 52.74%\n",
      "Epoch: 581, loss: 28906.379, training accuracy: 53.00%\n",
      "Epoch: 582, loss: 4515.328, training accuracy: 51.35%\n",
      "Epoch: 583, loss: 7535.837, training accuracy: 51.42%\n",
      "Epoch: 584, loss: 5472.527, training accuracy: 56.36%\n",
      "Epoch: 585, loss: 8177.908, training accuracy: 51.39%\n",
      "Epoch: 586, loss: 7816.153, training accuracy: 51.39%\n",
      "Epoch: 587, loss: 12356.486, training accuracy: 52.85%\n",
      "Epoch: 588, loss: 8740.562, training accuracy: 52.22%\n",
      "Epoch: 589, loss: 17232.197, training accuracy: 50.53%\n",
      "Epoch: 590, loss: 14003.086, training accuracy: 54.94%\n",
      "Epoch: 591, loss: 12389.111, training accuracy: 51.13%\n",
      "Epoch: 592, loss: 8475.631, training accuracy: 51.18%\n",
      "Epoch: 593, loss: 11554.949, training accuracy: 59.38%\n",
      "Epoch: 594, loss: 6725.404, training accuracy: 52.22%\n",
      "Epoch: 595, loss: 8103.579, training accuracy: 55.29%\n",
      "Epoch: 596, loss: 27471.275, training accuracy: 52.97%\n",
      "Epoch: 597, loss: 6372.204, training accuracy: 52.56%\n",
      "Epoch: 598, loss: 15127.170, training accuracy: 53.96%\n",
      "Epoch: 599, loss: 9158.133, training accuracy: 53.71%\n",
      "Epoch: 600, loss: 19015.289, training accuracy: 54.83%\n",
      "Epoch: 601, loss: 11433.495, training accuracy: 52.47%\n",
      "Epoch: 602, loss: 12732.412, training accuracy: 49.24%\n",
      "Epoch: 603, loss: 6049.403, training accuracy: 56.85%\n",
      "Epoch: 604, loss: 18787.111, training accuracy: 50.43%\n",
      "Epoch: 605, loss: 13560.508, training accuracy: 53.06%\n",
      "Epoch: 606, loss: 5786.383, training accuracy: 55.81%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 607, loss: 19195.463, training accuracy: 52.75%\n",
      "Epoch: 608, loss: 5343.174, training accuracy: 55.87%\n",
      "Epoch: 609, loss: 5824.980, training accuracy: 55.65%\n",
      "Epoch: 610, loss: 17520.125, training accuracy: 52.76%\n",
      "Epoch: 611, loss: 9458.879, training accuracy: 55.96%\n",
      "Epoch: 612, loss: 14593.074, training accuracy: 52.79%\n",
      "Epoch: 613, loss: 6488.737, training accuracy: 54.47%\n",
      "Epoch: 614, loss: 5199.720, training accuracy: 57.17%\n",
      "Epoch: 615, loss: 5645.634, training accuracy: 58.63%\n",
      "Epoch: 616, loss: 15268.133, training accuracy: 50.88%\n",
      "Epoch: 617, loss: 6980.137, training accuracy: 53.29%\n",
      "Epoch: 618, loss: 15248.080, training accuracy: 52.81%\n",
      "Epoch: 619, loss: 10889.516, training accuracy: 54.94%\n",
      "Epoch: 620, loss: 2454.378, training accuracy: 57.43%\n",
      "Epoch: 621, loss: 19341.969, training accuracy: 51.90%\n",
      "Epoch: 622, loss: 4538.867, training accuracy: 55.92%\n",
      "Epoch: 623, loss: 13809.246, training accuracy: 50.65%\n",
      "Epoch: 624, loss: 8601.844, training accuracy: 54.64%\n",
      "Epoch: 625, loss: 8085.634, training accuracy: 55.93%\n",
      "Epoch: 626, loss: 8523.652, training accuracy: 51.14%\n",
      "Epoch: 627, loss: 4238.636, training accuracy: 48.56%\n",
      "Epoch: 628, loss: 4793.068, training accuracy: 54.93%\n",
      "Epoch: 629, loss: 6185.434, training accuracy: 53.78%\n",
      "Epoch: 630, loss: 5775.653, training accuracy: 57.18%\n",
      "Epoch: 631, loss: 6977.369, training accuracy: 54.49%\n",
      "Epoch: 632, loss: 7802.010, training accuracy: 51.96%\n",
      "Epoch: 633, loss: 6649.660, training accuracy: 50.56%\n",
      "Epoch: 634, loss: 3396.979, training accuracy: 55.87%\n",
      "Epoch: 635, loss: 5689.527, training accuracy: 53.31%\n",
      "Epoch: 636, loss: 3953.790, training accuracy: 54.39%\n",
      "Epoch: 637, loss: 4176.928, training accuracy: 54.83%\n",
      "Epoch: 638, loss: 7832.971, training accuracy: 53.92%\n",
      "Epoch: 639, loss: 7243.934, training accuracy: 53.51%\n",
      "Epoch: 640, loss: 2731.692, training accuracy: 56.39%\n",
      "Epoch: 641, loss: 4204.438, training accuracy: 54.29%\n",
      "Epoch: 642, loss: 2645.766, training accuracy: 54.07%\n",
      "Epoch: 643, loss: 10454.445, training accuracy: 49.61%\n",
      "Epoch: 644, loss: 6867.560, training accuracy: 51.53%\n",
      "Epoch: 645, loss: 8046.569, training accuracy: 53.24%\n",
      "Epoch: 646, loss: 7177.565, training accuracy: 53.12%\n",
      "Epoch: 647, loss: 8629.458, training accuracy: 52.67%\n",
      "Epoch: 648, loss: 2835.927, training accuracy: 52.64%\n",
      "Epoch: 649, loss: 5090.865, training accuracy: 56.08%\n",
      "Epoch: 650, loss: 2890.084, training accuracy: 54.81%\n",
      "Epoch: 651, loss: 6083.878, training accuracy: 55.81%\n",
      "Epoch: 652, loss: 5449.292, training accuracy: 52.08%\n",
      "Epoch: 653, loss: 7378.708, training accuracy: 54.29%\n",
      "Epoch: 654, loss: 3347.097, training accuracy: 52.39%\n",
      "Epoch: 655, loss: 3897.700, training accuracy: 56.08%\n",
      "Epoch: 656, loss: 5857.902, training accuracy: 55.96%\n",
      "Epoch: 657, loss: 8363.428, training accuracy: 54.61%\n",
      "Epoch: 658, loss: 4332.601, training accuracy: 53.71%\n",
      "Epoch: 659, loss: 9319.600, training accuracy: 51.42%\n",
      "Epoch: 660, loss: 6289.156, training accuracy: 50.72%\n",
      "Epoch: 661, loss: 4038.500, training accuracy: 51.18%\n",
      "Epoch: 662, loss: 5165.870, training accuracy: 55.03%\n",
      "Epoch: 663, loss: 9330.498, training accuracy: 52.53%\n",
      "Epoch: 664, loss: 7003.972, training accuracy: 54.60%\n",
      "Epoch: 665, loss: 8678.406, training accuracy: 50.24%\n",
      "Epoch: 666, loss: 6815.079, training accuracy: 56.40%\n",
      "Epoch: 667, loss: 7043.793, training accuracy: 49.99%\n",
      "Epoch: 668, loss: 16438.754, training accuracy: 51.67%\n",
      "Epoch: 669, loss: 9588.451, training accuracy: 54.25%\n",
      "Epoch: 670, loss: 4177.077, training accuracy: 53.78%\n",
      "Epoch: 671, loss: 8262.638, training accuracy: 51.54%\n",
      "Epoch: 672, loss: 4838.663, training accuracy: 51.24%\n",
      "Epoch: 673, loss: 6426.812, training accuracy: 56.08%\n",
      "Epoch: 674, loss: 2734.573, training accuracy: 54.90%\n",
      "Epoch: 675, loss: 9440.426, training accuracy: 53.22%\n",
      "Epoch: 676, loss: 9119.294, training accuracy: 52.42%\n",
      "Epoch: 677, loss: 9077.877, training accuracy: 54.58%\n",
      "Epoch: 678, loss: 5880.318, training accuracy: 56.76%\n",
      "Epoch: 679, loss: 6549.807, training accuracy: 52.04%\n",
      "Epoch: 680, loss: 7685.859, training accuracy: 51.63%\n",
      "Epoch: 681, loss: 6285.842, training accuracy: 52.92%\n",
      "Epoch: 682, loss: 6533.603, training accuracy: 50.32%\n",
      "Epoch: 683, loss: 4395.658, training accuracy: 54.74%\n",
      "Epoch: 684, loss: 4332.427, training accuracy: 56.12%\n",
      "Epoch: 685, loss: 6274.510, training accuracy: 52.99%\n",
      "Epoch: 686, loss: 7928.420, training accuracy: 53.93%\n",
      "Epoch: 687, loss: 4804.093, training accuracy: 54.94%\n",
      "Epoch: 688, loss: 9643.373, training accuracy: 56.22%\n",
      "Epoch: 689, loss: 3577.857, training accuracy: 53.18%\n",
      "Epoch: 690, loss: 18707.033, training accuracy: 51.22%\n",
      "Epoch: 691, loss: 7325.608, training accuracy: 55.17%\n",
      "Epoch: 692, loss: 4645.542, training accuracy: 50.53%\n",
      "Epoch: 693, loss: 9254.743, training accuracy: 51.61%\n",
      "Epoch: 694, loss: 6058.378, training accuracy: 52.94%\n",
      "Epoch: 695, loss: 7485.996, training accuracy: 52.94%\n",
      "Epoch: 696, loss: 6728.690, training accuracy: 48.81%\n",
      "Epoch: 697, loss: 5483.684, training accuracy: 52.78%\n",
      "Epoch: 698, loss: 7592.032, training accuracy: 51.99%\n",
      "Epoch: 699, loss: 7488.423, training accuracy: 53.60%\n",
      "Epoch: 700, loss: 7339.739, training accuracy: 52.17%\n",
      "Epoch: 701, loss: 6390.891, training accuracy: 51.69%\n",
      "Epoch: 702, loss: 10769.093, training accuracy: 51.69%\n",
      "Epoch: 703, loss: 3248.730, training accuracy: 54.90%\n",
      "Epoch: 704, loss: 5866.034, training accuracy: 55.68%\n",
      "Epoch: 705, loss: 9462.855, training accuracy: 49.86%\n",
      "Epoch: 706, loss: 7315.778, training accuracy: 53.61%\n",
      "Epoch: 707, loss: 6293.833, training accuracy: 50.68%\n",
      "Epoch: 708, loss: 4889.344, training accuracy: 50.33%\n",
      "Epoch: 709, loss: 11032.564, training accuracy: 55.78%\n",
      "Epoch: 710, loss: 5496.847, training accuracy: 59.39%\n",
      "Epoch: 711, loss: 3835.017, training accuracy: 54.76%\n",
      "Epoch: 712, loss: 5504.558, training accuracy: 57.97%\n",
      "Epoch: 713, loss: 4422.551, training accuracy: 54.87%\n",
      "Epoch: 714, loss: 10776.543, training accuracy: 50.93%\n",
      "Epoch: 715, loss: 6249.187, training accuracy: 50.33%\n",
      "Epoch: 716, loss: 5001.482, training accuracy: 53.31%\n",
      "Epoch: 717, loss: 4282.921, training accuracy: 52.93%\n",
      "Epoch: 718, loss: 2638.410, training accuracy: 54.67%\n",
      "Epoch: 719, loss: 10516.975, training accuracy: 52.14%\n",
      "Epoch: 720, loss: 6014.772, training accuracy: 55.58%\n",
      "Epoch: 721, loss: 5149.548, training accuracy: 50.82%\n",
      "Epoch: 722, loss: 13382.183, training accuracy: 47.25%\n",
      "Epoch: 723, loss: 2714.681, training accuracy: 56.82%\n",
      "Epoch: 724, loss: 13200.389, training accuracy: 52.28%\n",
      "Epoch: 725, loss: 9506.460, training accuracy: 57.28%\n",
      "Epoch: 726, loss: 11262.399, training accuracy: 51.72%\n",
      "Epoch: 727, loss: 4846.011, training accuracy: 55.36%\n",
      "Epoch: 728, loss: 12601.188, training accuracy: 51.68%\n",
      "Epoch: 729, loss: 7955.985, training accuracy: 51.83%\n",
      "Epoch: 730, loss: 9249.005, training accuracy: 53.06%\n",
      "Epoch: 731, loss: 9465.416, training accuracy: 53.86%\n",
      "Epoch: 732, loss: 9439.284, training accuracy: 50.72%\n",
      "Epoch: 733, loss: 13351.676, training accuracy: 54.78%\n",
      "Epoch: 734, loss: 5895.780, training accuracy: 56.28%\n",
      "Epoch: 735, loss: 14041.752, training accuracy: 54.21%\n",
      "Epoch: 736, loss: 8607.904, training accuracy: 52.61%\n",
      "Epoch: 737, loss: 6819.987, training accuracy: 54.58%\n",
      "Epoch: 738, loss: 3897.844, training accuracy: 53.44%\n",
      "Epoch: 739, loss: 8180.775, training accuracy: 56.36%\n",
      "Epoch: 740, loss: 2758.318, training accuracy: 56.38%\n",
      "Epoch: 741, loss: 8538.681, training accuracy: 50.32%\n",
      "Epoch: 742, loss: 3643.110, training accuracy: 50.82%\n",
      "Epoch: 743, loss: 3406.277, training accuracy: 53.99%\n",
      "Epoch: 744, loss: 5135.313, training accuracy: 54.87%\n",
      "Epoch: 745, loss: 9190.972, training accuracy: 51.17%\n",
      "Epoch: 746, loss: 3959.649, training accuracy: 57.51%\n",
      "Epoch: 747, loss: 5988.194, training accuracy: 52.85%\n",
      "Epoch: 748, loss: 4138.487, training accuracy: 52.57%\n",
      "Epoch: 749, loss: 5361.757, training accuracy: 56.00%\n",
      "Epoch: 750, loss: 10763.076, training accuracy: 50.71%\n",
      "Epoch: 751, loss: 7320.987, training accuracy: 52.71%\n",
      "Epoch: 752, loss: 7201.707, training accuracy: 52.63%\n",
      "Epoch: 753, loss: 6711.962, training accuracy: 50.92%\n",
      "Epoch: 754, loss: 5817.840, training accuracy: 53.04%\n",
      "Epoch: 755, loss: 4604.556, training accuracy: 50.85%\n",
      "Epoch: 756, loss: 4625.282, training accuracy: 54.35%\n",
      "Epoch: 757, loss: 3583.399, training accuracy: 55.40%\n",
      "Epoch: 758, loss: 4965.466, training accuracy: 52.18%\n",
      "Epoch: 759, loss: 5872.370, training accuracy: 54.62%\n",
      "Epoch: 760, loss: 8217.947, training accuracy: 49.51%\n",
      "Epoch: 761, loss: 5218.115, training accuracy: 53.71%\n",
      "Epoch: 762, loss: 4533.170, training accuracy: 51.93%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 763, loss: 4426.671, training accuracy: 55.21%\n",
      "Epoch: 764, loss: 13062.443, training accuracy: 51.17%\n",
      "Epoch: 765, loss: 6550.985, training accuracy: 52.10%\n",
      "Epoch: 766, loss: 10304.779, training accuracy: 50.85%\n",
      "Epoch: 767, loss: 8212.349, training accuracy: 52.28%\n",
      "Epoch: 768, loss: 10220.693, training accuracy: 48.76%\n",
      "Epoch: 769, loss: 3325.174, training accuracy: 52.85%\n",
      "Epoch: 770, loss: 4773.762, training accuracy: 53.44%\n",
      "Epoch: 771, loss: 6247.668, training accuracy: 50.25%\n",
      "Epoch: 772, loss: 3805.100, training accuracy: 52.08%\n",
      "Epoch: 773, loss: 7504.094, training accuracy: 52.46%\n",
      "Epoch: 774, loss: 8114.439, training accuracy: 51.93%\n",
      "Epoch: 775, loss: 4480.206, training accuracy: 54.15%\n",
      "Epoch: 776, loss: 7632.646, training accuracy: 53.76%\n",
      "Epoch: 777, loss: 7991.769, training accuracy: 54.33%\n",
      "Epoch: 778, loss: 9798.615, training accuracy: 52.65%\n",
      "Epoch: 779, loss: 10934.605, training accuracy: 52.01%\n",
      "Epoch: 780, loss: 3959.929, training accuracy: 51.32%\n",
      "Epoch: 781, loss: 7210.027, training accuracy: 52.15%\n",
      "Epoch: 782, loss: 3874.632, training accuracy: 55.10%\n",
      "Epoch: 783, loss: 5863.667, training accuracy: 50.22%\n",
      "Epoch: 784, loss: 5285.496, training accuracy: 53.31%\n",
      "Epoch: 785, loss: 5302.791, training accuracy: 53.54%\n",
      "Epoch: 786, loss: 5334.184, training accuracy: 58.07%\n",
      "Epoch: 787, loss: 4972.888, training accuracy: 53.40%\n",
      "Epoch: 788, loss: 3469.880, training accuracy: 53.31%\n",
      "Epoch: 789, loss: 6353.252, training accuracy: 57.13%\n",
      "Epoch: 790, loss: 5292.565, training accuracy: 56.36%\n",
      "Epoch: 791, loss: 7176.307, training accuracy: 52.22%\n",
      "Epoch: 792, loss: 8220.029, training accuracy: 53.46%\n",
      "Epoch: 793, loss: 5054.632, training accuracy: 53.67%\n",
      "Epoch: 794, loss: 5387.152, training accuracy: 52.92%\n",
      "Epoch: 795, loss: 5331.168, training accuracy: 49.96%\n",
      "Epoch: 796, loss: 7226.356, training accuracy: 52.04%\n",
      "Epoch: 797, loss: 4228.014, training accuracy: 54.60%\n",
      "Epoch: 798, loss: 4753.230, training accuracy: 52.32%\n",
      "Epoch: 799, loss: 7496.607, training accuracy: 54.62%\n",
      "Epoch: 800, loss: 3240.700, training accuracy: 60.65%\n",
      "Epoch: 801, loss: 5361.504, training accuracy: 56.39%\n",
      "Epoch: 802, loss: 8935.944, training accuracy: 57.68%\n",
      "Epoch: 803, loss: 3259.914, training accuracy: 51.51%\n",
      "Epoch: 804, loss: 1861.610, training accuracy: 57.96%\n",
      "Epoch: 805, loss: 7228.377, training accuracy: 52.13%\n",
      "Epoch: 806, loss: 4176.090, training accuracy: 57.14%\n",
      "Epoch: 807, loss: 7629.359, training accuracy: 54.18%\n",
      "Epoch: 808, loss: 9673.492, training accuracy: 56.88%\n",
      "Epoch: 809, loss: 8215.736, training accuracy: 57.96%\n",
      "Epoch: 810, loss: 6943.376, training accuracy: 50.43%\n",
      "Epoch: 811, loss: 28139.424, training accuracy: 51.06%\n",
      "Epoch: 812, loss: 10830.832, training accuracy: 52.81%\n",
      "Epoch: 813, loss: 5338.982, training accuracy: 53.46%\n",
      "Epoch: 814, loss: 16923.496, training accuracy: 56.22%\n",
      "Epoch: 815, loss: 7000.662, training accuracy: 54.11%\n",
      "Epoch: 816, loss: 14203.707, training accuracy: 52.35%\n",
      "Epoch: 817, loss: 15769.360, training accuracy: 52.82%\n",
      "Epoch: 818, loss: 14374.582, training accuracy: 49.11%\n",
      "Epoch: 819, loss: 4306.475, training accuracy: 53.18%\n",
      "Epoch: 820, loss: 7037.780, training accuracy: 51.32%\n",
      "Epoch: 821, loss: 6522.039, training accuracy: 53.04%\n",
      "Epoch: 822, loss: 5800.083, training accuracy: 52.61%\n",
      "Epoch: 823, loss: 7742.900, training accuracy: 56.68%\n",
      "Epoch: 824, loss: 6743.142, training accuracy: 53.31%\n",
      "Epoch: 825, loss: 3392.435, training accuracy: 53.74%\n",
      "Epoch: 826, loss: 5600.717, training accuracy: 53.04%\n",
      "Epoch: 827, loss: 6711.244, training accuracy: 53.17%\n",
      "Epoch: 828, loss: 4939.694, training accuracy: 51.90%\n",
      "Epoch: 829, loss: 5107.081, training accuracy: 52.90%\n",
      "Epoch: 830, loss: 3024.713, training accuracy: 51.40%\n",
      "Epoch: 831, loss: 6050.504, training accuracy: 55.19%\n",
      "Epoch: 832, loss: 6081.384, training accuracy: 51.38%\n",
      "Epoch: 833, loss: 9330.774, training accuracy: 56.71%\n",
      "Epoch: 834, loss: 5594.614, training accuracy: 52.72%\n",
      "Epoch: 835, loss: 4255.836, training accuracy: 54.90%\n",
      "Epoch: 836, loss: 9325.064, training accuracy: 54.65%\n",
      "Epoch: 837, loss: 8566.682, training accuracy: 54.29%\n",
      "Epoch: 838, loss: 4799.984, training accuracy: 53.51%\n",
      "Epoch: 839, loss: 24141.582, training accuracy: 49.63%\n",
      "Epoch: 840, loss: 10261.643, training accuracy: 55.83%\n",
      "Epoch: 841, loss: 27179.379, training accuracy: 53.61%\n",
      "Epoch: 842, loss: 66174.992, training accuracy: 49.04%\n",
      "Epoch: 843, loss: 33780.891, training accuracy: 53.54%\n",
      "Epoch: 844, loss: 31772.363, training accuracy: 51.21%\n",
      "Epoch: 845, loss: 29395.793, training accuracy: 54.03%\n",
      "Epoch: 846, loss: 14820.271, training accuracy: 52.13%\n",
      "Epoch: 847, loss: 40399.531, training accuracy: 55.83%\n",
      "Epoch: 848, loss: 9051.486, training accuracy: 55.11%\n",
      "Epoch: 849, loss: 20694.824, training accuracy: 55.75%\n",
      "Epoch: 850, loss: 17217.650, training accuracy: 52.07%\n",
      "Epoch: 851, loss: 23723.090, training accuracy: 53.19%\n",
      "Epoch: 852, loss: 9144.100, training accuracy: 54.31%\n",
      "Epoch: 853, loss: 19824.574, training accuracy: 54.76%\n",
      "Epoch: 854, loss: 15569.412, training accuracy: 50.56%\n",
      "Epoch: 855, loss: 20248.420, training accuracy: 52.14%\n",
      "Epoch: 856, loss: 6957.750, training accuracy: 52.94%\n",
      "Epoch: 857, loss: 8335.229, training accuracy: 55.62%\n",
      "Epoch: 858, loss: 30667.109, training accuracy: 49.46%\n",
      "Epoch: 859, loss: 8082.376, training accuracy: 53.26%\n",
      "Epoch: 860, loss: 18841.727, training accuracy: 50.78%\n",
      "Epoch: 861, loss: 33309.461, training accuracy: 51.75%\n",
      "Epoch: 862, loss: 30855.285, training accuracy: 48.32%\n",
      "Epoch: 863, loss: 30464.416, training accuracy: 59.29%\n",
      "Epoch: 864, loss: 69078.047, training accuracy: 49.64%\n",
      "Epoch: 865, loss: 9665.688, training accuracy: 57.29%\n",
      "Epoch: 866, loss: 15115.514, training accuracy: 55.26%\n",
      "Epoch: 867, loss: 26350.531, training accuracy: 53.75%\n",
      "Epoch: 868, loss: 24876.996, training accuracy: 50.03%\n",
      "Epoch: 869, loss: 5038.986, training accuracy: 55.29%\n",
      "Epoch: 870, loss: 9395.287, training accuracy: 56.17%\n",
      "Epoch: 871, loss: 19610.400, training accuracy: 52.97%\n",
      "Epoch: 872, loss: 7965.047, training accuracy: 52.51%\n",
      "Epoch: 873, loss: 10936.271, training accuracy: 54.10%\n",
      "Epoch: 874, loss: 17162.762, training accuracy: 51.69%\n",
      "Epoch: 875, loss: 7431.798, training accuracy: 53.54%\n",
      "Epoch: 876, loss: 16823.750, training accuracy: 55.28%\n",
      "Epoch: 877, loss: 18528.256, training accuracy: 51.61%\n",
      "Epoch: 878, loss: 25284.574, training accuracy: 54.50%\n",
      "Epoch: 879, loss: 8461.119, training accuracy: 52.11%\n",
      "Epoch: 880, loss: 11218.734, training accuracy: 53.26%\n",
      "Epoch: 881, loss: 9312.908, training accuracy: 55.93%\n",
      "Epoch: 882, loss: 18063.324, training accuracy: 50.26%\n",
      "Epoch: 883, loss: 9520.076, training accuracy: 54.22%\n",
      "Epoch: 884, loss: 20057.027, training accuracy: 51.44%\n",
      "Epoch: 885, loss: 31852.770, training accuracy: 56.89%\n",
      "Epoch: 886, loss: 26980.777, training accuracy: 54.56%\n",
      "Epoch: 887, loss: 31537.744, training accuracy: 54.03%\n",
      "Epoch: 888, loss: 15375.639, training accuracy: 51.44%\n",
      "Epoch: 889, loss: 22196.340, training accuracy: 50.18%\n",
      "Epoch: 890, loss: 9321.672, training accuracy: 52.71%\n",
      "Epoch: 891, loss: 7241.219, training accuracy: 57.25%\n",
      "Epoch: 892, loss: 19532.820, training accuracy: 52.06%\n",
      "Epoch: 893, loss: 6318.025, training accuracy: 53.83%\n",
      "Epoch: 894, loss: 4895.017, training accuracy: 53.31%\n",
      "Epoch: 895, loss: 16137.182, training accuracy: 53.28%\n",
      "Epoch: 896, loss: 8181.505, training accuracy: 52.90%\n",
      "Epoch: 897, loss: 9957.021, training accuracy: 58.21%\n",
      "Epoch: 898, loss: 9651.596, training accuracy: 55.18%\n",
      "Epoch: 899, loss: 7241.495, training accuracy: 53.89%\n",
      "Epoch: 900, loss: 27948.980, training accuracy: 53.65%\n",
      "Epoch: 901, loss: 3131.051, training accuracy: 53.28%\n",
      "Epoch: 902, loss: 9990.322, training accuracy: 55.07%\n",
      "Epoch: 903, loss: 9617.588, training accuracy: 51.03%\n",
      "Epoch: 904, loss: 6359.544, training accuracy: 54.28%\n",
      "Epoch: 905, loss: 3509.451, training accuracy: 53.26%\n",
      "Epoch: 906, loss: 5701.747, training accuracy: 52.50%\n",
      "Epoch: 907, loss: 6042.274, training accuracy: 52.54%\n",
      "Epoch: 908, loss: 8786.347, training accuracy: 52.67%\n",
      "Epoch: 909, loss: 4998.478, training accuracy: 56.44%\n",
      "Epoch: 910, loss: 11196.879, training accuracy: 50.57%\n",
      "Epoch: 911, loss: 3696.965, training accuracy: 55.62%\n",
      "Epoch: 912, loss: 11126.551, training accuracy: 54.67%\n",
      "Epoch: 913, loss: 22317.961, training accuracy: 52.11%\n",
      "Epoch: 914, loss: 5953.286, training accuracy: 51.46%\n",
      "Epoch: 915, loss: 8257.484, training accuracy: 57.21%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 916, loss: 6297.994, training accuracy: 54.62%\n",
      "Epoch: 917, loss: 5765.147, training accuracy: 52.64%\n",
      "Epoch: 918, loss: 10088.203, training accuracy: 49.17%\n",
      "Epoch: 919, loss: 3369.886, training accuracy: 52.06%\n",
      "Epoch: 920, loss: 4384.009, training accuracy: 54.47%\n",
      "Epoch: 921, loss: 4609.985, training accuracy: 52.54%\n",
      "Epoch: 922, loss: 2731.385, training accuracy: 50.75%\n",
      "Epoch: 923, loss: 4590.994, training accuracy: 50.03%\n",
      "Epoch: 924, loss: 10806.908, training accuracy: 51.79%\n",
      "Epoch: 925, loss: 3998.376, training accuracy: 58.54%\n",
      "Epoch: 926, loss: 5894.812, training accuracy: 53.92%\n",
      "Epoch: 927, loss: 9877.222, training accuracy: 54.28%\n",
      "Epoch: 928, loss: 6543.425, training accuracy: 54.29%\n",
      "Epoch: 929, loss: 2245.740, training accuracy: 53.97%\n",
      "Epoch: 930, loss: 3521.054, training accuracy: 53.72%\n",
      "Epoch: 931, loss: 5685.462, training accuracy: 53.61%\n",
      "Epoch: 932, loss: 6003.835, training accuracy: 49.97%\n",
      "Epoch: 933, loss: 5557.993, training accuracy: 52.39%\n",
      "Epoch: 934, loss: 6120.175, training accuracy: 50.38%\n",
      "Epoch: 935, loss: 4889.694, training accuracy: 58.01%\n",
      "Epoch: 936, loss: 4109.186, training accuracy: 54.18%\n",
      "Epoch: 937, loss: 6924.629, training accuracy: 53.57%\n",
      "Epoch: 938, loss: 3463.140, training accuracy: 53.60%\n",
      "Epoch: 939, loss: 6734.782, training accuracy: 53.00%\n",
      "Epoch: 940, loss: 7137.312, training accuracy: 54.79%\n",
      "Epoch: 941, loss: 5616.189, training accuracy: 56.46%\n",
      "Epoch: 942, loss: 6307.190, training accuracy: 52.53%\n",
      "Epoch: 943, loss: 4464.710, training accuracy: 53.49%\n",
      "Epoch: 944, loss: 10470.789, training accuracy: 55.57%\n",
      "Epoch: 945, loss: 8166.780, training accuracy: 52.21%\n",
      "Epoch: 946, loss: 6779.281, training accuracy: 54.87%\n",
      "Epoch: 947, loss: 4242.059, training accuracy: 54.83%\n",
      "Epoch: 948, loss: 4307.782, training accuracy: 54.61%\n",
      "Epoch: 949, loss: 13426.518, training accuracy: 53.82%\n",
      "Epoch: 950, loss: 11308.046, training accuracy: 53.89%\n",
      "Epoch: 951, loss: 9335.099, training accuracy: 51.74%\n",
      "Epoch: 952, loss: 7921.165, training accuracy: 55.83%\n",
      "Epoch: 953, loss: 2962.226, training accuracy: 53.39%\n",
      "Epoch: 954, loss: 6146.617, training accuracy: 53.25%\n",
      "Epoch: 955, loss: 11981.491, training accuracy: 50.60%\n",
      "Epoch: 956, loss: 18068.445, training accuracy: 51.51%\n",
      "Epoch: 957, loss: 3436.615, training accuracy: 57.33%\n",
      "Epoch: 958, loss: 2608.417, training accuracy: 50.83%\n",
      "Epoch: 959, loss: 10690.770, training accuracy: 53.72%\n",
      "Epoch: 960, loss: 4475.140, training accuracy: 55.58%\n",
      "Epoch: 961, loss: 12426.281, training accuracy: 51.01%\n",
      "Epoch: 962, loss: 7728.846, training accuracy: 54.18%\n",
      "Epoch: 963, loss: 4242.584, training accuracy: 51.96%\n",
      "Epoch: 964, loss: 8480.615, training accuracy: 52.67%\n",
      "Epoch: 965, loss: 7979.849, training accuracy: 52.50%\n",
      "Epoch: 966, loss: 7728.339, training accuracy: 50.88%\n",
      "Epoch: 967, loss: 7862.780, training accuracy: 55.06%\n",
      "Epoch: 968, loss: 4709.479, training accuracy: 53.24%\n",
      "Epoch: 969, loss: 8197.243, training accuracy: 51.57%\n",
      "Epoch: 970, loss: 3878.265, training accuracy: 55.79%\n",
      "Epoch: 971, loss: 3310.882, training accuracy: 52.47%\n",
      "Epoch: 972, loss: 8040.548, training accuracy: 50.76%\n",
      "Epoch: 973, loss: 4978.384, training accuracy: 50.24%\n",
      "Epoch: 974, loss: 4967.305, training accuracy: 54.12%\n",
      "Epoch: 975, loss: 4350.257, training accuracy: 53.46%\n",
      "Epoch: 976, loss: 4823.638, training accuracy: 50.24%\n",
      "Epoch: 977, loss: 10981.988, training accuracy: 52.60%\n",
      "Epoch: 978, loss: 4752.395, training accuracy: 55.62%\n",
      "Epoch: 979, loss: 9466.892, training accuracy: 54.79%\n",
      "Epoch: 980, loss: 4864.992, training accuracy: 54.26%\n",
      "Epoch: 981, loss: 4271.178, training accuracy: 50.64%\n",
      "Epoch: 982, loss: 10720.263, training accuracy: 51.79%\n",
      "Epoch: 983, loss: 4222.224, training accuracy: 57.22%\n",
      "Epoch: 984, loss: 5675.502, training accuracy: 54.69%\n",
      "Epoch: 985, loss: 7745.907, training accuracy: 48.35%\n",
      "Epoch: 986, loss: 4229.558, training accuracy: 55.25%\n",
      "Epoch: 987, loss: 5287.184, training accuracy: 56.61%\n",
      "Epoch: 988, loss: 2488.465, training accuracy: 56.63%\n",
      "Epoch: 989, loss: 6199.166, training accuracy: 54.57%\n",
      "Epoch: 990, loss: 4897.307, training accuracy: 49.72%\n",
      "Epoch: 991, loss: 6426.862, training accuracy: 54.26%\n",
      "Epoch: 992, loss: 8824.523, training accuracy: 55.68%\n",
      "Epoch: 993, loss: 5880.403, training accuracy: 53.71%\n",
      "Epoch: 994, loss: 8616.223, training accuracy: 52.89%\n",
      "Epoch: 995, loss: 2713.869, training accuracy: 54.99%\n",
      "Epoch: 996, loss: 4510.091, training accuracy: 55.15%\n",
      "Epoch: 997, loss: 11024.273, training accuracy: 54.11%\n",
      "Epoch: 998, loss: 13679.168, training accuracy: 52.85%\n",
      "Epoch: 999, loss: 13605.039, training accuracy: 57.89%\n",
      "Average validation set accuracy over 10 iterations is 53.52%\n"
     ]
    }
   ],
   "source": [
    "# run the training\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allocator_type = 'BFC'\n",
    "config.gpu_options.allow_growth = True\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.90\n",
    "#sess = tf.Session(config=config)\n",
    "#K.set_session(sess)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "epochs = 1000\n",
    "with tf.Session(config=config) as sess:\n",
    "    sess.run(init_op)\n",
    "    sess.run(training_init_op)\n",
    "    for i in range(epochs):\n",
    "        l, _, acc = sess.run([loss, optimizer, accuracy])\n",
    "        \n",
    "        print(\"Epoch: {}, loss: {:.3f}, training accuracy: {:.2f}%\".format(i, l, acc * 100))\n",
    "    # now setup the validation run\n",
    "    valid_iters = 10\n",
    "    # re-initialize the iterator, but this time with validation data\n",
    "    sess.run(validation_init_op)\n",
    "    avg_acc = 0\n",
    "    for i in range(valid_iters):\n",
    "        acc = sess.run([accuracy])\n",
    "        avg_acc += acc[0]\n",
    "    print(\"Average validation set accuracy over {} iterations is {:.2f}%\".format(valid_iters,(avg_acc / valid_iters) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0 % 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vox_preprocess(vox):\n",
    "    vox_shape = vox.shape\n",
    "    vox = np.reshape(vox, (-1, vox_shape[-1]))\n",
    "    vox = scale(vox, axis=0)\n",
    "    return np.reshape(vox, vox_shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29640, 120, 120)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andrii\\Anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\utils\\validation.py:590: DataConversionWarning: Data with input dtype uint8 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "num_classees=2\n",
    "a = vox_preprocess(y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_train = x_train.astype('float32') \n",
    "imgs_val = x_test.astype('float32')\n",
    "imgs_train /= 255. \n",
    "imgs_val /= 255.\n",
    "    # imgs_train_1 = imgs_train[:, :, :, 0]\n",
    "    # imgs_train_1 = imgs_train_1[..., np.newaxis]\n",
    "    # print(imgs_train_1.shape)\n",
    "    \n",
    "    # define data preparation\n",
    "batch_size = 12\n",
    "datagen = ImageDataGenerator(featurewise_center=True,\n",
    "                                featurewise_std_normalization=True,\n",
    "                                 rotation_range=20, # rescale=1./255,\n",
    "                                 width_shift_range=0.1, height_shift_range=0.1,\n",
    "                                 horizontal_flip=True, vertical_flip=True,                                 \n",
    "                                 shear_range=0.2, fill_mode='nearest')\n",
    "#==============================================================================\n",
    "#     datagen = ImageDataGenerator(rotation_range=20, # rescale=1./255,\n",
    "#                                  width_shift_range=0.1, height_shift_range=0.1,\n",
    "#                                  horizontal_flip=True, vertical_flip=True,                                 \n",
    "#                                  shear_range=0.2, fill_mode='nearest')\n",
    "#==============================================================================\n",
    "# fit parameters from data\n",
    "datagen.fit(imgs_train)     \n",
    "# datagen.fit(imgs_train_1)\n",
    "train_generator = datagen.flow(imgs_train, Y, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the augmentation configuration we will use for testing:\n",
    "val_datagen = ImageDataGenerator(featurewise_center=True,\n",
    "                                     featurewise_std_normalization=True)\n",
    "                                     # rescale=1./255)\n",
    "# val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "    \n",
    "# fit parameters from data\n",
    "val_datagen.fit(imgs_val) \n",
    "# val_datagen.fit(imgs_train)   \n",
    "val_generator = val_datagen.flow(imgs_val, Y_test, batch_size=batch_size)\n",
    "# datagen.standardize(imgs_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tr_0=np.reshape(y_train[150], 120*120).astype('int32')\n",
    "y_tr_0 = np_utils.to_categorical(y_tr_0, 2)\n",
    "y_tr_0=y_tr_0[np.newaxis, ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_labels(labels):\n",
    "    new_labels=np.empty((labels.shape[0], 120*120, 2), dtype='int32')\n",
    "    \n",
    "    for i,y in enumerate(labels):\n",
    "        im=np.reshape(y, 120*120).astype('int32')\n",
    "        im=np_utils.to_categorical(im, 2)\n",
    "        im=im[np.newaxis, ...]\n",
    "        new_labels[i]=im\n",
    "    return new_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y=make_labels(y_train)\n",
    "Y_test=make_labels(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tr_0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coef(y_true, y_pred):\n",
    "    y_true_f = K.flatten(y_true.astype('float32'))\n",
    "    y_pred_f = K.flatten(y_pred.astype('float32'))\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    \n",
    "    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "\n",
    "def dice_coef_loss(y_true, y_pred):\n",
    "    return -dice_coef(y_true, y_pred)\n",
    "\n",
    "def cnnBRATsInit_unet():  \n",
    "    # the output layer is softmax, loss should be categorical_crossentropy\n",
    "    # Downsampling 2^4 = 16 ==> 240/16 = 15\n",
    "    \n",
    "    inputs = Input((120, 120, 3))        \n",
    "    # inputs = Input((img_rows, img_cols, 4))\n",
    "    \n",
    "    # Normalize all data before extracting features\n",
    "    input_nor = BatchNormalization()(inputs)\n",
    "    # Block 1\n",
    "    conv1 = Conv2D(64, kernel_size=(3, 3), activation='relu', padding='same',\n",
    "                   kernel_regularizer=l2(0.01))(input_nor)\n",
    "    # conv1 = BatchNormalization()(conv1)\n",
    "    # conv1 = Dropout(0.2)(conv1)\n",
    "    conv1 = Conv2D(64, kernel_size=(3, 3), activation='relu', padding='same',\n",
    "                   kernel_regularizer=l2(0.01))(conv1)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    # conv1 = Dropout(0.2)(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)    \n",
    "    \n",
    "    # Block 2\n",
    "    conv2 = Conv2D(128, kernel_size=(3, 3), activation='relu', padding='same',\n",
    "                   kernel_regularizer=l2(0.01))(pool1) \n",
    "    # conv2 = BatchNormalization()(conv2)\n",
    "    # conv2 = Dropout(0.5)(conv2)\n",
    "    conv2 = Conv2D(128, kernel_size=(3, 3), activation='relu', padding='same',\n",
    "                   kernel_regularizer=l2(0.01))(conv2)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    # conv2 = Dropout(0.5)(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "    # pool2 = Dropout(0.5)(pool2)\n",
    "    \n",
    "    # this MaxPooling2D has the same result with the above line.\n",
    "    # pool2 = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding='same')(conv2)\n",
    "    \n",
    "    # W_new = (W - F + 2P)/S + 1\n",
    "    # W_new = (120 - 3 + 2*1)/2 + 1 = 60.5 --> 60 (round down)\n",
    "    # P = 1, because the padding parameter is 'same', the size of W has no change\n",
    "    # before divide by Stride parameter\n",
    "    \n",
    "    \n",
    "    # Block 3\n",
    "    conv3 = Conv2D(256, kernel_size=(3, 3), activation='relu', padding='same', \n",
    "                   kernel_regularizer=l2(0.01))(pool2)  \n",
    "    # conv3 = BatchNormalization()(conv3)\n",
    "    # conv3 = Dropout(0.5)(conv3)\n",
    "    conv3 = Conv2D(256, kernel_size=(3, 3), activation='relu', padding='same', \n",
    "                   kernel_regularizer=l2(0.01))(conv3)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    # conv3 = Dropout(0.5)(conv3)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "    # pool3 = Dropout(0.5)(pool3)\n",
    "    # dropout1 = Dropout(0.5)(pool3)\n",
    "    \n",
    "    # Block 4\n",
    "    conv4 = Conv2D(512, kernel_size=(3, 3), activation='relu', padding='same', \n",
    "                   kernel_regularizer=l2(0.01))(pool3)\n",
    "    # conv4 = BatchNormalization()(conv4)\n",
    "    # conv4 = Dropout(0.5)(conv4)\n",
    "    conv4 = Conv2D(512, kernel_size=(3, 3), activation='relu', padding='same', \n",
    "                   kernel_regularizer=l2(0.01))(conv4)\n",
    "    conv4 = BatchNormalization()(conv4)\n",
    "    # conv4 = Dropout(0.5)(conv4)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
    "    # pool4 = Dropout(0.5)(pool4)\n",
    "    # dropout2 = Dropout(0.5)(pool4)\n",
    "    \n",
    "    # Block 5\n",
    "    conv5 = Conv2D(1024, kernel_size=(3, 3), activation='relu', padding='same',\n",
    "                   kernel_regularizer=l2(0.01))(pool4)\n",
    "    # conv5 = BatchNormalization()(conv5)\n",
    "    # conv5 = Dropout(0.5)(conv5)\n",
    "    conv5 = Conv2D(1024, kernel_size=(3, 3), activation='relu', padding='same',\n",
    "                   kernel_regularizer=l2(0.01))(conv5)\n",
    "    conv5 = BatchNormalization()(conv5)\n",
    "    # pool5 = MaxPooling2D(pool_size=(3, 3), strides=(1, 1), padding='same')(conv5)\n",
    "    # conv5 = Dropout(0.5)(conv5)\n",
    "    \n",
    "    # Block 6\n",
    "    # axis = -1 same as axis = last dim order, in this case axis = 3\n",
    "    up6 = concatenate([Conv2DTranspose(512, kernel_size=(2, 2), strides=(2, 2), \n",
    "                                       padding='same')(conv5), conv4], axis=3)\n",
    "    conv6 = Conv2D(512, kernel_size=(3, 3), activation='relu', padding='same',\n",
    "                   kernel_regularizer=l2(0.01))(up6) \n",
    "    # conv6 = BatchNormalization()(conv6)\n",
    "    # conv6 = Dropout(0.5)(conv6)\n",
    "    conv6 = Conv2D(512, kernel_size=(3, 3), activation='relu', padding='same',\n",
    "                   kernel_regularizer=l2(0.01))(conv6)\n",
    "    conv6 = BatchNormalization()(conv6)\n",
    "    # conv6 = Dropout(0.5)(conv6)\n",
    "    \n",
    "    # Block 7\n",
    "    up7 = concatenate([Conv2DTranspose(256, kernel_size=(2, 2), strides=(2, 2), \n",
    "                                       padding='same')(conv6), conv3], axis=3)\n",
    "    conv7 = Conv2D(256, kernel_size=(3, 3), activation='relu', padding='same',\n",
    "                   kernel_regularizer=l2(0.01))(up7) \n",
    "    # conv7 = BatchNormalization()(conv7)\n",
    "    # conv7 = Dropout(0.5)(conv7)\n",
    "    conv7 = Conv2D(256, kernel_size=(3, 3), activation='relu', padding='same',\n",
    "                   kernel_regularizer=l2(0.01))(conv7)\n",
    "    conv7 = BatchNormalization()(conv7)\n",
    "    # conv7 = Dropout(0.5)(conv7)\n",
    "    \n",
    "    # Block 8\n",
    "    up8 = concatenate([Conv2DTranspose(128, kernel_size=(2, 2), strides=(2, 2), \n",
    "                                       padding='same')(conv7), conv2], axis=3)\n",
    "    conv8 = Conv2D(128, kernel_size=(3, 3), activation='relu', padding='same',\n",
    "                   kernel_regularizer=l2(0.01))(up8)\n",
    "    # conv8 = BatchNormalization()(conv8)\n",
    "    # conv8 = Dropout(0.5)(conv8)\n",
    "    conv8 = Conv2D(128, kernel_size=(3, 3), activation='relu', padding='same',\n",
    "                   kernel_regularizer=l2(0.01))(conv8)\n",
    "    conv8 = BatchNormalization()(conv8)\n",
    "    # conv8 = Dropout(0.5)(conv8)\n",
    "    \n",
    "    # Block 9\n",
    "    up9 = concatenate([Conv2DTranspose(64, kernel_size=(2, 2), strides=(2, 2), \n",
    "                                       padding='same')(conv8), conv1], axis=3)\n",
    "    conv9 = Conv2D(64, kernel_size=(3, 3), activation='relu', padding='same',\n",
    "                   kernel_regularizer=l2(0.01))(up9) \n",
    "    # conv9 = BatchNormalization()(conv9)\n",
    "    # conv9 = Dropout(0.5)(conv9)\n",
    "    conv9 = Conv2D(64, kernel_size=(3, 3), activation='relu', padding='same',\n",
    "                   kernel_regularizer=l2(0.01))(conv9)\n",
    "    conv9 = BatchNormalization()(conv9)\n",
    "    # conv9 = Dropout(0.5)(conv9)\n",
    "    \n",
    "    # Block 10\n",
    "    conv10 = Conv2D(nclasses, kernel_size=(1, 1), padding='same')(conv9)\n",
    "    \n",
    "    # Block 10\n",
    "    # curr_channels = nclasses\n",
    "    _, curr_width, curr_height, curr_channels = conv10._keras_shape\n",
    "    out_conv10 = Reshape((curr_width * curr_height, curr_channels))(conv10)\n",
    "    act_soft = Activation('softmax')(out_conv10)\n",
    "    \n",
    "    model = Model(inputs=[inputs], outputs=[act_soft])\n",
    "\n",
    "\n",
    "    sgd = SGD(lr=0.0001, decay=0.01, momentum=0.9, nesterov=True)    \n",
    "    # sgd = SGD(lr=0.0001, decay=1, momentum=0.9, nesterov=True)\n",
    "    model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy']) \n",
    "    \n",
    "    # adam = Adam(lr=0.0001)\n",
    "    # model.compile(optimizer=Adam(lr=0.0001), loss='categorical_crossentropy', metrics=['accuracy']) \n",
    "    \n",
    "    # model.compile(optimizer='sgd', loss=dice_coef_loss, metrics=[dice_coef])\n",
    "    \n",
    "    # model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#epochs = 3\n",
    "\n",
    "#Adam = keras.optimizers.Adam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon= 0.001,decay=0.00, amsgrad=False)\n",
    "#my_model.compile(optimizer=Adam, loss ='categorical_crossentropy', metrics=['accuracy'])\n",
    "my_model = cnnBRATsInit_unet()\n",
    "my_model.fit_generator(train_generator, steps_per_epoch=(imgs_train.shape[0] // batch_size) + 1, epochs=20,                        \n",
    "                                  verbose=1,\n",
    "                                  validation_data=val_generator,\n",
    "                                  validation_steps=(imgs_val.shape[0] // batch_size) + 1)\n",
    "\n",
    "score = my_model.evaluate(val_generator, batch_size=32)\n",
    "my_model.metrics_names , score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model = cnnBRATsInit_unet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tr = np_utils.to_categorical(y_train)\n",
    "y_te = np_utils.to_categorical(y_test)\n",
    "\n",
    "#y_tr=y_train.reshape(-1,2)\n",
    "#y_te=y_train.reshape(-1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr = x_train.reshape(-1, 3)\n",
    "x_te = x_test.reshape(-1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_tr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dims = y_train.shape\n",
    "classes = len(set(y_train[134].flatten())) # get num classes from first image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_model = keras.applications.vgg16.VGG16(include_top=False, weights='imagenet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = Input(shape=(120,120,3),name = 'image_input')\n",
    "\n",
    "#Use the generated model \n",
    "input_nor = BatchNormalization()(input)\n",
    "output_vgg16_conv = vgg_model(input_nor)\n",
    "\n",
    "#Add the fully-connected layers \n",
    "x = Flatten(name='flatten')(output_vgg16_conv)\n",
    "x = Dense(4096, activation='relu', name='fc1')(x)\n",
    "x = Dense(4096, activation='relu', name='fc2')(x)\n",
    "x = Dense(5, activation='softmax', name='predictions')(x)\n",
    "\n",
    "#Create your own model \n",
    "my_model = Model(input=input, output=x)\n",
    "\n",
    "#In the summary, weights and layers from VGG part will be hidden, but they will be fit during the training\n",
    "#my_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input = Input(shape=(120,120,3),name = 'image_input')\n",
    "vgg_model = keras.applications.vgg16.VGG16(include_top=False, weights='imagenet', pooling=None)\n",
    "#Use the generated model \n",
    "input_nor = BatchNormalization()(input)\n",
    "output_vgg16_conv = vgg_model(input_nor)\n",
    "my_model=vgg_model\n",
    "\n",
    "my_model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['accuracy']) \n",
    "\n",
    "\n",
    "my_model.fit_generator(train_generator, steps_per_epoch=(imgs_train.shape[0] // batch_size) + 1, epochs=20,                        \n",
    "                                  verbose=1,\n",
    "                                  validation_data=val_generator,\n",
    "                                  validation_steps=(imgs_val.shape[0] // batch_size) + 1)\n",
    "\n",
    "score = my_model.evaluate(val_generator, batch_size=32)\n",
    "my_model.metrics_names , score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
